[["index.html", "DfT R Cookbook ", " DfT R Cookbook Isi Avbulimen, Hannah Bougdah, Tamsin Forbes, Jack Marks, Tim Taylor, Francesca Bryden 2023-01-17 "],["why-do-we-need-another-book.html", "Chapter 1 Why do we need another book? 1.1 Coding standards 1.2 Data 1.3 Work in progress", " Chapter 1 Why do we need another book? R is a very flexible programming language, which inevitably means there are lots of ways to achieve the same result. This is true of all programming languages, but is particularly exaggerated in R which makes use of ‘meta-programming’. For example, here is how to calculate a new variable using standard R and filter on a variable: # Calculate kilometers per litre from miles per gallon mtcars$kpl &lt;- mtcars$mpg * 0.425144 # Select cars with a horsepower greater than 250 &amp; show only mpg and kpl columns mtcars[mtcars$hp &gt; 250, c(&quot;car&quot;, &quot;mpg&quot;, &quot;kpl&quot;)] car mpg kpl 29 15.8 6.717275 31 15.0 6.377160 Here’s the same thing using {tidyverse} style R: mtcars %&gt;% # Calculate kilometers per litre dplyr::mutate( kpl = mpg * 0.425144 ) %&gt;% # Filter cars with a horsepower greater than 250 dplyr::filter( hp &gt; 250 ) %&gt;% # Take only the car, mpg, and newly created kpl columns dplyr::select(car, mpg, kpl) car mpg kpl 29 15.8 6.717275 31 15.0 6.377160 These coding styles are quite different. As people write more code across the Department, it will become increasingly important that code can be handed over to other R users. It is much easier to pick up code written by others if it uses the same coding style you are familiar with. This is the main motivation for this book, to establish a way of coding that represents a sensible default for those who are new to R that is readily transferable across DfT. 1.1 Coding standards Related to this, the Data Science team maintain a coding standards document, that outlines some best practices when writing R code. This is not prescriptive and goes beyond the scope of this document, but might be useful for managing your R projects. 1.2 Data The data used in this book is all derived from opensource data. As well as being availble fro the data folder on the github site here you can also find the larger data sets at the links below. Road Safety Data Search and Rescue Helicopter data; SARH0112 record level data download available under All data at the bottom of this webpage. Pokemon data, not sure of original source, but borrowed from Matt Dray here Port data; the port data can be downloaded by clicking on the table named PORT0499 1.3 Work in progress This book is not static - new chapters can be added and current chapters can be amended. Please let us know if you have suggestions by raising an issue here. "],["basics.html", "Chapter 2 The R basics 2.1 R family 2.2 DfT R/RStudio - subject to change 2.3 RStudio IDE 2.4 Projects 2.5 R memory 2.6 A note on rounding 2.7 Assignment operators &lt;- vs = 2.8 Arithmetic operators 2.9 Relational operators 2.10 Logical operators 2.11 Vectors", " Chapter 2 The R basics 2.1 R family A few of the common R relations are R is the programming language, born in 1997, based on S, honest. RStudio is a useful integrated development environment (IDE) that makes it cleaner to write, run and organise R code. Rproj is the file extension for an R project, essentially a working directory marker, shortens file paths, keeps everything relevant to your project together and easy to reference. packages are collections of functions written to make specific tasks easier, eg the {stringr} package contains functions to work with strings. Packages are often referred to as libraries in other programming languages. .R is the file extension for a basic R script in which anything not commented out with # is code that is run. .Rmd is the file extension for Rmarkdown an R package useful for producing reports. A .Rmd script is different to a .R script in that the default is text rather than code. Code is placed in code chunks - similar to how a Jupyter Notebook looks. 2.2 DfT R/RStudio - subject to change Which version of R/RStudio should I use at DfT? A good question. Currently the ‘best’ version of R we have available on network is linked to RStudio version 11453. This can be accessed via the Citrix app on the Windows 10 devices, or via Citrix desktop. The local version of RStudio on the Windows 10 devices is currently unusable (user testing is ongoing to change this). There is also a 11423 version of RStudio available which uses slightly older versions of packages. 2.3 RStudio IDE The RStudio integrated development environment has some very useful features which make writing and organising code a lot easier. It’s divided into 3 panes; 2.3.1 Left (bottom left if you have scripts open) this is the Console it shows you what code has been run and outputs. 2.3.2 Top right; Environment, and other tabs Environment tab shows what objects have been created in the global environment in the current session. Connections tab will show any connections you have set up this session, for example, to an SQL server. 2.3.3 Bottom right Files tab shows what directory you are in and the files there. Plots tab shows all the plot outputs created this session, you can navigate through them. Packages tab shows a list of installed packages, if the box in front of the package name is checked then this package has been loaded this session. Help tab can be used to search for help on a topic/package function, it also holds any output from ?function_name help command that has been run in the console, again you can navigate through help topics using the left and right arrows. Viewer tab can be used to view local web content. For some pictures have a look at DfE’s R Training Course getting started with rstudio Or Matt Dray’s Beginner R Featuring Pokemon: the RStudio interface 2.3.4 Other handy buttons in RStudio IDE top left new script icon; blank page with green circle and white cross. top right project icon; 3D transparent light blue cube with R. Use this to create and open projects. 2.3.5 RStudio menu bar a few pointers View contains Zoom In Zoom Out Tools -&gt; Global Options contains many useful setting tabs such as Appearance where you can change the RStudio theme, and Code -&gt; Display where you can set a margin vertical guideline (default is 80 characters). 2.4 Projects Why you should work in an R project, how to set up and project happiness. See this section of Beginner R Featuring Pokemon by Matt Dray. 2.4.1 Folders When you set up a project it is good practise to include separate folders for different types of files such as data; for the data your R code is using output; for files creates by your code R; all your code files, eg .R, .Rmd images 2.4.2 sessionInfo() Include a saved file of the sessionInfo() output, this command prints out the versions of all the packages currently loaded. This information is essential when passing on code as packages can be updated and code breaking changes made. 2.5 R memory R works in RAM, so its memory is only as good as the amount of RAM you have - however this should be sufficient for most tasks. More info in the Memory chapter of Advanced R by Hadley Wickham here. 2.6 A note on rounding For rounding numerical values we have the base function round(x, digits = 0). This rounds the value of the first argument to the specified number of decimal places (default 0). round(c(-1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5)) ## [1] -2 0 0 2 2 4 4 For example, note that 1.5 and 2.5 both round to 2, which is probably not what you were expecting, this is generally referred to as ‘round half to even’. The round() documentation explains all (?round) Note that for rounding off a 5, the IEC 60559 standard (see also ‘IEEE 754’) is expected to be used, ‘go to the even digit’. Therefore round(0.5) is 0 and round(-1.5) is -2. However, this is dependent on OS services and on representation error (since e.g. 0.15 is not represented exactly, the rounding rule applies to the represented number and not to the printed number, and so round(0.15, 1) could be either 0.1 or 0.2). To implement what we consider normal rounding we can use the {janitor} package and the function round_half_up library(janitor) ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test janitor::round_half_up(c(-1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5)) ## [1] -2 -1 1 2 3 4 5 If we do not have access to the package (or do not want to depend on the package) then we can implement^[see stackoverflow round_half_up_v2 &lt;- function(x, digits = 0) { posneg &lt;- sign(x) z &lt;- abs(x) * 10 ^ digits z &lt;- z + 0.5 z &lt;- trunc(z) z &lt;- z / 10 ^ digits z * posneg } round_half_up_v2(c(-1.5, -0.5, 0.5, 1.5, 2.5, 3.5, 4.5)) ## [1] -2 -1 1 2 3 4 5 2.7 Assignment operators &lt;- vs = To assign or to equal? These are not always the same thing. In R to assign a value to a variable it is advised to use &lt;- rather than =. The latter is generally used for setting parameters inside functions, e.g., my_string &lt;- stringr::str_match(string = \"abc\", pattern = \"a\"). More on assignment operators here. 2.8 Arithmetic operators addition 1 + 2 ## [1] 3 subtraction 5 - 4 ## [1] 1 multiplication 2 * 2 ## [1] 4 division 3 / 2 ## [1] 1.5 exponent 3 ^ 2 ## [1] 9 modulus (remainder on divsion) 14 %% 6 ## [1] 2 integer division 50 %/% 8 ## [1] 6 2.9 Relational operators less than 3.14 &lt; 3.142 ## [1] TRUE greater than 3.14159 &gt; 3 ## [1] TRUE less than or equal to 3 &lt;= 3.14 ## [1] TRUE 3.14 &lt;= 3.14 ## [1] TRUE greater than or equal to 3 &gt;= 3.14 ## [1] FALSE 3.14 &gt;= 3.14 ## [1] TRUE equal to 3 == 3.14159 ## [1] FALSE not equal to 3 != 3.14159 ## [1] TRUE 2.10 Logical operators Logical operations are possible only for numeric, logical or complex types. Note that 0 (or complex version 0 + 0i) is equivalent to FALSE, and all other numbers (numeric or complex) are equivalent to TRUE. not ! x &lt;- c(TRUE, 0, FALSE, -4) !x ## [1] FALSE TRUE TRUE FALSE element-wise and &amp; y &lt;- c(3.14, FALSE, TRUE, 0) x &amp; y ## [1] TRUE FALSE FALSE FALSE first element and &amp;&amp; x &amp;&amp; y ## Warning in x &amp;&amp; y: &#39;length(x) = 4 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## Warning in x &amp;&amp; y: &#39;length(x) = 4 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## [1] TRUE element-wise or | x | y ## [1] TRUE FALSE TRUE TRUE first element or || z &lt;- c(0, FALSE, 8) y || z ## Warning in y || z: &#39;length(x) = 4 &gt; 1&#39; in coercion to &#39;logical(1)&#39; ## [1] TRUE 2.11 Vectors 2.11.1 Types There are four main atomic vector types that you are likely to come across when using R1; logical (TRUE or FALSE), double (3.142), integer (2L) and character (\"Awesome\") v1 &lt;- TRUE typeof(v1) ## [1] &quot;logical&quot; v1 &lt;- FALSE typeof(v1) ## [1] &quot;logical&quot; v2 &lt;- 1.5 typeof(v2) ## [1] &quot;double&quot; v2 &lt;- 1 typeof(v2) ## [1] &quot;double&quot; # integer values must be followed by an L to be stored as integers v3 &lt;- 2 typeof(v3) ## [1] &quot;double&quot; v3 &lt;- 2L typeof(v3) ## [1] &quot;integer&quot; v4 &lt;- &quot;Awesome&quot; typeof(v4) ## [1] &quot;character&quot; As well as the atomic vector types you will often encounter two other vector types; Date and factor . As well as some notes here this book also contains fuller sections on both Chapter 5 Working with dates and times Chapter 6 Working with factors Factor vectors are used to represent categorical data. They are actually integer vectors with two additional attributes, levels and class. At this stage it is not worth worrying too much about what attributes are, but is suffiecient to understand that, for factors, the levels attribute gives the possible categories, and combined with the integer values works much like a lookup table. The class attribute is just “factor”. ratings &lt;- factor(c(&quot;good&quot;, &quot;bad&quot;, &quot;bad&quot;, &quot;amazing&quot;)) typeof(ratings) ## [1] &quot;integer&quot; attributes(ratings) ## $levels ## [1] &quot;amazing&quot; &quot;bad&quot; &quot;good&quot; ## ## $class ## [1] &quot;factor&quot; Date vectors are just vectors of class double with an additional class attribute set as “Date”. DfT_birthday &lt;- lubridate::as_date(&quot;1919-08-14&quot;) typeof(DfT_birthday) ## [1] &quot;double&quot; attributes(DfT_birthday) ## $class ## [1] &quot;Date&quot; If we remove the class using unclass() we can reveal the value of the double, which is the number of days since “1970-01-01”2, since DfT’s birthday is before this date, the double is negative. unclass(DfT_birthday) ## [1] -18403 2.11.2 Conversion between atomic vector types Converting between the atomic vector types is done using the as.character, as.integer, as.logical and as.double functions. value &lt;- 1.5 as.integer(value) ## [1] 1 as.character(value) ## [1] &quot;1.5&quot; as.logical(value) ## [1] TRUE Where it is not possible to convert a value you will get a warning message value &lt;- &quot;z&quot; as.integer(value) ## Warning: NAs introduced by coercion ## [1] NA When combining different vector types, coercion will obey the following hierarchy: character, double, integer, logical. typeof(c(9.9, 3L, &quot;pop&quot;, TRUE)) ## [1] &quot;character&quot; typeof(c(9.9, 3L, TRUE)) ## [1] &quot;double&quot; typeof(c(3L, TRUE)) ## [1] &quot;integer&quot; typeof(TRUE) ## [1] &quot;logical&quot; technically there are more, see https://adv-r.hadley.nz/vectors-chap.html#atomic-vectors↩︎ a special date known as the Unix Epoch↩︎ "],["packages.html", "Chapter 3 R Packages 3.1 What are packages? 3.2 Best practice in using packages 3.3 Recommended packages 3.4 Package management", " Chapter 3 R Packages This chapter discusses best practice for using packages in your R code, and gives recommended packages for a range of applications. 3.1 What are packages? While base R can be used to achieve some impressive results, the vast majority of coding in R is done using one or more packages. Packages (often also called libraries) are shareable units of code which are written, published and maintained by other R coders. The scope of each individual package varies massively; some perform a single function, whereas others are large and underpin many core aspects of everyday R usage. The huge variety of packages is one of the reasons that R is so successful: it allows you to benefit from the work of others by downloading their packages. The most common way to install R packages is using the Comprehensive R Archive Network, or CRAN, which you do via an install.packages() call e.g. install.packages(&quot;data.table&quot;) install.packages(&quot;dplyr&quot;) 3.2 Best practice in using packages While it’s very easy to get started with packages, there are a few best practice tips which will make it easier to manage how you use packages in a consistent, replicable and transparent way. 3.2.1 Calling packages You can tell your code when you want to make use of the contents of a package by making a library() call in your code. This needs to be done before you run any code which references that package e.g. ## This code will produce an error toTitleCase(&quot;hello&quot;) library(tools) ## This code will run fine library(tools) toTitleCase(&quot;hello&quot;) Your code should always call all libraries it uses at the very beginning of the code, even if the package won’t be used until much later. This allows users to know what packages they need in any piece of code. Your code also shouldn’t include any install.packages() calls. Forcing code users to install packages will make significant changes to their coding environment, and may even break other code for them. If you need to ensure they have the right packages installed, you should use package version control such as renv. To ensure you’ve called all necessary packages in your code, it is helpful run your code in a fresh R instance (CTRL+SHIFT+F10 to refresh your environment) and rerun your code, to check it isn’t dependent on any libraries pre-loaded in your environment. 3.2.2 Core package usage The large number of R packages available means that it’s common for there to be multiple packages which perform essentially the same functions. Use of multiple similar packages in this way makes your code difficult to use for beginners, and is more likely to cause bugs and other errors. To minimise this, your code should start from a basis of a core group of common packages, and only add additional or different packages where they fulfill functions which can’t be met by the core packages. This chapter includes a number of suggestions of core packages which are well-supported and easy to use, and that you should to aim to use where possible. 3.2.3 Using Github packages While there are a large number of packages available to download on CRAN, there are many more that people have released on other platforms, particularly Github. This is usually because they don’t want to go through the stringent review process which CRAN requires, or their package is still under development. Packages can be easily downloaded and used from Github using the remotes::install_github() function, and this allows you to make use of a wider code base in your work. However, Github packages should be used with caution: They have not been peer reviewed like CRAN packages, so are more likely to contain problems, errors or even malicious code. Bugs may not be regularly (or ever) corrected. There is no guarantee the package will be maintained. You should always use a CRAN package in preference to a Github alternative where one is available. You should also be cautious of downloading and using Github packages where the author is not known to you, and you should ensure you have a good understanding of the code in the package and what it does before installing or using it. 3.2.4 Using packages instead of stand-alone functions Packages offer a number of benefits over just including stand-alone functions in your code. They increase the consistency of code usage, preventing you from using different iterations of the same function in different situations. They are also regularly tested and checked, and you can be confident they are performing their function as intended. You should generally always aim to use a function from a common package rather than writing your own where possible. 3.2.5 Heavy dependency packages Sometimes when calling libraries, it is easy to accidentally create a lot of heavy dependencies for your code. These are packages which in turn are dependent on a large number of other packages, requiring you to install many packages before running the code and increasing the points of failure. When introducing new packages into your code, it is always worth checking if the function you are calling can be sourced in a more lightweight way, e.g. ##Tidyverse is an example of a heavy dependency which is not really required for simply using the pipe function library(tidyverse) mtcars %&gt;% head() ##A much better alternative if other code from the tidyverse is not required, is to call the package that the pipe originates from instead (magrittr), which has few dependencies library(magrittr) mtcars %&gt;% head() 3.2.6 Specifying packages when using functions As well as calling libraries at the start of your code, you can also specify what package a function comes from every time you use it using “double colon” formatting: ##Calling a function without referencing the package str_trim(&quot; test &quot;) ##Calling a function including the package using &quot;double colon&quot; notation. stringr::str_trim(&quot; test &quot;) While this format is slightly more time-consuming to write, it has a number of advantages: Easy to understand which package each function comes from, particularly useful for unusual functions Greater specificity reduces the risk of function conflicts across packages Individual lines of code will run without needing to make library calls Easy setup of package dependencies by packages such as renv Allows code to be converted to a package easily 3.3 Recommended packages Due to the diversity of packages available for R, and the range of tasks people successfully complete with them, it is impossible to provide a definitive list of all the packages people should be using. Rather, this section aims to capture a short list of packages which perform functions common to most analytical projects, such as data reading, cleaning and visualisation. You should expect to add more specialist packages to your projects in most cases, but shouldn’t aim to substitute the below packages for others without strong rationale. 3.3.1 Tidyverse The tidyverse is the most ubiquitous set of packages used in R; to the point that most people will use tidyverse calls more frequently than they use base R. This set of packages is designed to work with tidy data, and perform a wide range of functions around data cleaning and use. These packages are widely used and supported across DfT, and best practice is to use these packages in preference to either alternatives or base R. While it is possible to call all of the packages within the tidyverse using library(tidyverse), this is rarely recommended as it is a heavy dependency. Instead, call the individual packages from within the tidyverse that you will use. This includes: ggplot2: data visualisation in charts and simple maps dplyr: data manipulation and processing tidyr: data tidying and organisation to produce tidy datasets purrr: tools for working with functions and vectors tibble: enhanced version of data frames stringr: manipulation of strings lubridate: manipulation of dates DBI and odbc: connecting to SQL databases httr, rvest, jsonlite and xml2: tools for extracting data from the web. 3.3.2 Non-tidyverse packages External to the tidyverse there are also a small number of core packages you will likely want to use when appropriate. openxlsx: reading and writing Excel files. Although readxl is a tidyverse alternative, openxlsx offers significantly enhanced functionality for changing the format and appearance of Excel documents, and is therefore a preferred alternative within DfT. data.table: fast reading and processing of very large datasets. The style of this package can be hard to pick up, so the tidyverse dtplyr may be preferred as it retains the dplyr structure. shiny: create HTML dashboards and apps within R Rmarkdown: create dynamic reports 3.3.3 DfT-specific packages In addition, there are a growing number of DfT-specific packages which have been released on our organisational Github. These have been designed to address-DfT specific gaps in the packages available, and are safe to download and use on network laptops. dftplotr: ggplot2 add-on which allows you to create charts with DfT themeing and colour palettes slidepackr: create HTML slidepacks using a DfT template dftutils: utility functions for everyday data processing 3.3.4 Specialist packages As previously mentioned, this isn’t intended to be an exhaustive list of packages, and you should expect to supplement it with specialist packages which are required for your work. If you have the choice of multiple packages to complete a goal, you should always aim to select one which meets these criteria where possible: Good documentation and community support for use Still maintained Available as a CRAN package 3.4 Package management As previously mentioned, it is bad practice to include install.package() calls in your code. However, it is difficult to ensure that code will be functional for multiple users without controlling the packages and package versions which users have. The solution to this is use of a package manager. These add-ons to R keep a record of the exact version of each package used in the code on a project-basis, and that collection of packages can be easily installed by any new user to the project. The best available package manager for R is renv, a marked improvement from packrat. To get started with renv on any project, you just need to run: renv::init() This will create all of the files you need to get started with renv, and will also make note of any existing dependencies your project has. Once your renv project is initialised, you can use libraries in it as normal, installing and using them as needed. To make a record of the package versions you have used, you run: renv::snapshot() And to retrieve this list of packages and install them in a new instance of the project, use: renv::restore() Further details on using renv can be found in the very comprehensive documentation "],["data-import.html", "Chapter 4 Data Importing/Exporting and interaction with other programmes 4.1 Libraries 4.2 Navigating folders 4.3 .rds 4.4 .csv 4.5 .xlsx and .xls 4.6 Exporting to .xlsx 4.7 .sav 4.8 SQL 4.9 GCP", " Chapter 4 Data Importing/Exporting and interaction with other programmes This chapter is for code examples of data importing/exporting and interactions with other programmes and databases. 4.1 Libraries library(tidyverse) library(fs) #cross-platform file systems operations (based on libuv C library) library(knitr) #general purpose tool for dynamic report generation in R library(kableExtra) #presentation of complex tables with customizable styles library(DT) #presentation of tables (wrapper of JavaScript library DataTables) library(DBI) #database connection library(dbplyr) #database connection library(haven) #for importing/exporting SPSS, Stata, and SAS files library(bigrquery) #connecting to GCP BigQuery 4.2 Navigating folders A couple of pointers to navigate from your working directory, which, if you’re using R projects (it is highly recommended that you do) will be wherever the .Rproj file is located 4.2.1 Down To navigate down folders use /. The path given below saves the file my_df.csv in the data folder, which itself is inside the monthly_work folder readr::write_csv( x = my_dataframe , path = &quot;monthly_work/data/my_df.csv&quot; ) 4.2.2 Up To go up a folder use ../. In particular you may need to do this when running Rmarkdown files. Rmarkdown files use their location as the working directory. If you have created an R folder, say, to stash all your scripts in, and a data folder to stash your data files in, then you will need to go up, before going down… The path below goes up one folder, then into the data folder, where the lookup_table.csv is located. lookup_table &lt;- readr::read_csv( file = &quot;../data/lookup_table.csv&quot; ) 4.3 .rds .rds is R’s native file format, any object you create in R can be saved as a .rds file. The functions readRDS and saveRDS are base R functions. #not run saveRDS( object = my_model #specify the R object you want to save , file = &quot;2019_0418_my_model.rds&quot; #give it a name, don&#39;t forget the file extension ) 4.4 .csv We use the functions read_csv and write_csv from the {readr} package (which is part of the {tidyverse}). These are a little bit cleverer than their base counterparts, however, this cleverness can catch you out. The file messy_pokemon_data.csv contains pokemon go captures data which has been deliberately messed up a bit. read_csv imputes the column specification from the first 1000 rows, which is fine if your first 1000 rows are representative of the data type. If not then subsequent data that can’t be coerced into the imputed data type will be replaced with NA. Looking at the column specification below notice that read_csv has recognised time_first_capture as a time type, but not date_first_capture as date type. Given the information that combat_power should be numeric we can see that something is also amiss here as read_csv has guessed character type for this column. pokemon &lt;- readr::read_csv( file = &quot;data/messy_pokemon_data.csv&quot; ) ## Rows: 696 Columns: 11 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (7): species, combat_power, weight_bin, height_bin, fast_attack, charge... ## dbl (3): hit_points, weight_kg, height_m ## time (1): time_first_capture ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Let’s have a quick look at some data from these columns pokemon %&gt;% dplyr::select(species, combat_power, date_first_capture, time_first_capture) %&gt;% dplyr::arrange(desc(combat_power)) %&gt;% head() species combat_power date_first_capture time_first_capture electabuzz P962 29/04/2001 08:20:10 pidgey P95 27/11/1969 21:59:32 drowzee P613 18/07/1968 10:36:38 bulbasaur P577 17 June 1997 09:17:17 drowzee P542 04/07/1928 21:54:04 drowzee P518 06/09/1950 17:01:18 The pokemon dataset has less than 1000 rows so read_csv has ‘seen’ the letters mixed in with some of the numbers in the combat_power column. It has guessed at character type because everything it has read in the column can be coerced to character type. What if there are more than 1000 rows? For example, say you have a numeric column, but there are some letters prefixed to the numbers in some of the post-row-1000 rows. These values are still meaningful to you, and with some data wrangling you can extract the actual numbers. Unfortunately read_csv has guessed at type double based on the first 1000 rows and since character type cannot be coerced into double, these values will be replaced with NA. If you have messy data like this the best thing to do is to force read_csv to read in as character type to preserve all values as they appear, you can then sort out the mess yourself. You can specify the column data type using the col_types argument. Below I have used a compact string of abbreviations (c = character, d = double, D = date, t = time) to specify the column types, see the help at ?read_csv or the {readr} vignette for the full list. You can see I got many parsing failures, which I can access with problems(), which is a data frame of the values that read_csv was unable to coerce into the type I specified, and so has replaced with NA. pokemon &lt;- readr::read_csv( file = &quot;data/messy_pokemon_data.csv&quot; , col_types = &quot;cdddcdcccDt&quot; ) ## Warning: One or more parsing issues, call `problems()` on your data frame for details, ## e.g.: ## dat &lt;- vroom(...) ## problems(dat) # c = character, d = double, D = Date, t = time tibble::glimpse(pokemon) ## Rows: 696 ## Columns: 11 ## $ species &lt;chr&gt; &quot;abra&quot;, &quot;abra&quot;, &quot;bellsprout&quot;, &quot;bellsprout&quot;, &quot;bellsp… ## $ combat_power &lt;dbl&gt; 101, 81, 156, 262, 389, 433, 628, 161, 135, 420, 12… ## $ hit_points &lt;dbl&gt; 20, 16, 32, 44, 50, 59, 68, 33, 29, 51, 26, 60, 15,… ## $ weight_kg &lt;dbl&gt; 17.18, 25.94, 5.85, 5.42, 3.40, 6.67, 3.84, 1.13, 1… ## $ weight_bin &lt;chr&gt; &quot;normal&quot;, &quot;extra_large&quot;, &quot;extra_large&quot;, &quot;extra_larg… ## $ height_m &lt;dbl&gt; 0.85, 1.00, 0.80, 0.82, 0.66, 0.84, 0.78, 0.39, 0.8… ## $ height_bin &lt;chr&gt; &quot;normal&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;… ## $ fast_attack &lt;chr&gt; &quot;zen_headbutt&quot;, &quot;zen_headbutt&quot;, &quot;acid&quot;, &quot;acid&quot;, &quot;vi… ## $ charge_attack &lt;chr&gt; &quot;shadow_ball&quot;, &quot;shadow_ball&quot;, &quot;sludge_bomb&quot;, &quot;sludg… ## $ date_first_capture &lt;date&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ time_first_capture &lt;time&gt; 20:59:33, 10:18:40, 08:06:55, 11:18:28, 21:11:42, … Let’s take a look at the problems. problems(pokemon) %&gt;% head() row col expected actual file 2 10 date in ISO8601 31 May 1977 /home/runner/work/R-cookbook/R-cookbook/data/messy_pokemon_data.csv 3 10 date in ISO8601 24 February 1973 /home/runner/work/R-cookbook/R-cookbook/data/messy_pokemon_data.csv 4 10 date in ISO8601 21 June 1924 /home/runner/work/R-cookbook/R-cookbook/data/messy_pokemon_data.csv 5 10 date in ISO8601 01 August 1925 /home/runner/work/R-cookbook/R-cookbook/data/messy_pokemon_data.csv 6 10 date in ISO8601 06 August 1952 /home/runner/work/R-cookbook/R-cookbook/data/messy_pokemon_data.csv 7 10 date in ISO8601 17 January 1915 /home/runner/work/R-cookbook/R-cookbook/data/messy_pokemon_data.csv And since I know that there are problems with combat_power let’s take a look there. problems(pokemon) %&gt;% dplyr::filter(col == &quot;combat_power&quot;) %&gt;% head() row col expected actual file The problems() feature in read_csv is super useful, it helps you isolate the problem data so you can fix it. Other arguments within read_csv that I will just mention, with their default settings are col_names = TRUE: the first row on the input is used as the column names. na = c(\"\", \"NA\"): the default values to interpret as NA. trim_ws = TRUE: by default trims leading/trailing white space. skip = 0: number of lines to skip before reading data. guess_max = min(1000, n_max): maximum number of records to use for guessing column type. NB the bigger this is the longer it will take to read in the data. 4.5 .xlsx and .xls Excel workbooks come in many shapes and sizes. You may have one or many worksheets in one or many workbooks, there may only be certain cells that you are interested in. Below are a few examples of how to cope with these variations using functions from {readxl} and {purrr} to iterate over either worksheets and/or workbooks, the aim being to end up with all the data in a single tidy dataframe. 4.5.1 Single worksheet - single workbook The simplest combination, you are interested in one rectangular dataset in a particular worksheet in one workbook. Leaving the defaults works fine on this dataset. Note that readxl::read_excel detects if the file is .xlsx or .xls and behaves accordingly. readxl::read_excel(path = &quot;data/port0499.xlsx&quot;) %&gt;% head() year polu_majorport polu_region direction cargo_group cargo_category cargo_description tonnage units 2000 Aberdeen Africa (excl. Mediterranean) Inwards Other General Cargo 92 Iron and steel products 1.153177 0 2000 Aberdeen Africa (excl. Mediterranean) Outwards Other General Cargo 92 Iron and steel products 2.406102 0 2000 Aberdeen Africa (excl. Mediterranean) Outwards Other General Cargo 99 Other general cargo &amp; containers &lt;20’ 3.664545 0 2000 Aberdeen America Inwards Other General Cargo 91 Forestry products 108.771082 0 2000 Aberdeen America Inwards Other General Cargo 92 Iron and steel products 11.045082 0 2000 Aberdeen America Outwards Other General Cargo 99 Other general cargo &amp; containers &lt;20’ 12.397106 0 Let’s set a few of the other arguments, run ?read_excel in the console to see the full list. readxl::read_excel( path = &quot;data/port0499.xlsx&quot; , sheet = 1 #number or name of sheet, default is first sheet , col_names = TRUE #default , col_types = &quot;text&quot; #a single type will recycle to all columns, specify each using character vector of the same length eg c(&quot;numeric&quot;, &quot;text&quot;, ...) ) %&gt;% head() year polu_majorport polu_region direction cargo_group cargo_category cargo_description tonnage units 2000 Aberdeen Africa (excl. Mediterranean) Inwards Other General Cargo 92 Iron and steel products 1.15317721340056 0 2000 Aberdeen Africa (excl. Mediterranean) Outwards Other General Cargo 92 Iron and steel products 2.4061025320368699 0 2000 Aberdeen Africa (excl. Mediterranean) Outwards Other General Cargo 99 Other general cargo &amp; containers &lt;20’ 3.6645448610128639 0 2000 Aberdeen America Inwards Other General Cargo 91 Forestry products 108.771081940982 0 2000 Aberdeen America Inwards Other General Cargo 92 Iron and steel products 11.045081750930599 0 2000 Aberdeen America Outwards Other General Cargo 99 Other general cargo &amp; containers &lt;20’ 12.397106259006 0 4.5.2 Single worksheet - many workbooks For example, you collect pokemon go capture data from many different players, the data all has the same structure and you want to read it in and row bind into a single dataframe in R. knitr::include_graphics(&quot;image/pokemon_player.png&quot;) The code below collects the names of the 3 excel workbooks using fs::dir_ls, and, as these are not the only files in that folder, I’ve specified them using regular expressions (regex). Then we use purrr::map_dfr to iterate and rowbind over this list of files, applying the function we supply, that is readxl::read_excel. Since we are only reading a single worksheet per workbook we don’t need to supply any arguments to readxl:read_excel, the defaults will work fine, each workbook path is piped in, in turn. The .id argument in purrr:map_dfr adds the file path into a new column, which we have named “player” in this instance. The “dfr” in map_dfr refers to the output “data-frame-rowbound”. pokemon &lt;- fs::dir_ls(path = &quot;data&quot;, regex = &quot;pokemon_player_.\\\\.xlsx$&quot;) %&gt;% purrr::map_dfr(.f = readxl::read_excel, .id = &quot;player&quot;) tibble::glimpse(pokemon) ## Rows: 15 ## Columns: 10 ## $ player &lt;chr&gt; &quot;data/pokemon_player_a.xlsx&quot;, &quot;data/pokemon_player_a.xls… ## $ species &lt;chr&gt; &quot;krabby&quot;, &quot;geodude&quot;, &quot;venonat&quot;, &quot;parasect&quot;, &quot;eevee&quot;, &quot;ni… ## $ combat_power &lt;dbl&gt; 51, 85, 129, 171, 172, 10, 234, 20, 173, 157, 249, 28, 2… ## $ hit_points &lt;dbl&gt; 15, 23, 38, 32, 37, 11, 33, 20, 26, 49, 50, 10, 13, 35, … ## $ weight_kg &lt;dbl&gt; 5.82, 20.88, 20.40, 19.20, 4.18, 6.21, 73.81, 5.28, 0.53… ## $ weight_bin &lt;chr&gt; &quot;normal&quot;, &quot;normal&quot;, &quot;extra_small&quot;, &quot;extra_small&quot;, &quot;extra… ## $ height_m &lt;dbl&gt; 0.36, 0.37, 0.92, 0.87, 0.25, 0.36, 1.52, 0.30, 0.49, 0.… ## $ height_bin &lt;chr&gt; &quot;normal&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;norma… ## $ fast_attack &lt;chr&gt; &quot;mud_shot&quot;, &quot;rock_throw&quot;, &quot;confusion&quot;, &quot;bug_bite&quot;, &quot;tack… ## $ charge_attack &lt;chr&gt; &quot;vice_grip&quot;, &quot;rock_tomb&quot;, &quot;poison_fang&quot;, &quot;x-scissor&quot;, &quot;b… Using DT::datatable for ease of viewing we can see that all 5 rows of data from each of the 3 workbooks has been read in, rowbound, and an id column has been added showing the workbook path. DT::datatable(data = pokemon) Note that the regex argument in fs::dir_ls is applied to the full file path so if I had tried to specify that the file name starts with “pokemon” by front anchoring it using “^pokemon” this would return no results, since the full name is actually “data/pokemon…”. Helpful regex links below. regex cheatsheet stringr cheatsheet including regex 4.5.3 Many worksheets - single workbook You have a single workbook, but it contains many worksheets of interest, each containing rectangular data with the same structure. For example, you have a workbook containing pokemon go captures data, where each different data collection point has its own sheet. The data structure, column names and data types are consistent. You want to read in and combine these data into a single dataframe. The code below sets the location of the workbook and puts this in the object path. It then collects the names of all the sheets in that workbook using readxl::excel_sheets. Next purrr::set_names sets these names in a vector so that they can be used in the next step. This vector of names is implicitly assigned to the .x argument in purrr::map_dfr as it is the first thing passed to it. This means we can refer to it as .x in the function we are iterating, in this case readxl::read_excel. Finally, an id column is included, made up of the sheet names and named “sheet”. The output is a single dataframe with all the sheets row bound together. path &lt;- &quot;data/multi_tab_messy_pokemon_data.xlsx&quot; pokemon_collections &lt;- readxl::excel_sheets(path = path) %&gt;% purrr::set_names() %&gt;% purrr::map_dfr( ~ readxl::read_excel(path = path, sheet = .x) , .id = &quot;sheet&quot; ) DT::datatable(data = pokemon_collections) 4.5.4 Many worksheets - many workbooks Now we can use the above two solutions to combine data from many worksheets spread across many workbooks. As before, the data is rectangular and has the same structure. For example, you receive a workbook every month, containing pokemon go captures data, and each data collection point has its own sheet. knitr::include_graphics(&quot;image/pokemon_collection_point.png&quot;) We create a function to import and combine the sheets from a single workbook, and then iterate this function over all the workbooks using purrr::map_df. #function to combine sheets from a single workbook read_and_combine_sheets &lt;- function(path){ readxl::excel_sheets(path = path) %&gt;% purrr::set_names() %&gt;% purrr::map_df( ~ readxl::read_excel(path = path, sheet = .x) , .id = &quot;sheet&quot; ) } #code to iterate over many workbooks pokemon_monthly_collections &lt;- fs::dir_ls( path = &quot;data&quot;, regex = &quot;pokemon_2019\\\\d{2}\\\\.xlsx$&quot;) %&gt;% purrr::map_df( read_and_combine_sheets , .id = &quot;month&quot; ) DT::datatable(data = pokemon_monthly_collections) 4.5.5 Non-rectangular data - single worksheet - single workbook You have received some kind of data entry form that has been done in excel in a more human readable, rather than machine readable, format. Some of the cells contain instructions and admin data so you only want the data held in specific cells. This is non-rectangular data, that is, the data of interest is dotted all over the place. In this example we have pet forms, and the data of interest is in cells B2, D5 and E8 only. Here’s an image of what the data looks like. knitr::include_graphics(&quot;image/pet_form.png&quot;) Let’s see what we get if we naively try to read it in. readxl::read_excel( path = &quot;data/pet_form_1.xlsx&quot; ) %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(full_width = F, position = &quot;left&quot;) Name: Tiddles …3 …4 …5 NA NA NA NA NA NA NA NA NA NA NA NA Age: 2 NA NA NA NA NA NA NA NA NA NA NA NA NA NA Species: cat It’s not what we wanted, let’s try again, now using the range argument readxl::read_excel( path = &quot;data/pet_form_1.xlsx&quot; , col_names = FALSE , range = &quot;A2:B2&quot; ) %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(full_width = F, position = &quot;left&quot;) …1 …2 Name: Tiddles The range argument helps, we have picked up one bit of the data, and its name. The range argument uses the {cellranger} package which allows you to refer to ranges in Excel files in Excel style. However, we have 3 disconnected data points, we need to iterate, so it’s {purrr} to the rescue once more. The code below demonstrates explicitly that the .x argument in purrr::map_dfr takes the vector of things that will be iterated over in the supplied function. In this case we are giving the range argument of readxl::read_excel three individual cells to iterate over. These will then be rowbound so we end up with a single dataframe comprising a single column, named “cells”, containing 3 rows. pet_details &lt;- purrr::map_dfr( .x = c(&quot;B2&quot;, &quot;D5&quot;, &quot;E8&quot;) , ~ readxl::read_excel( path = &quot;data/pet_form_1.xlsx&quot; , range = .x , col_names = &quot;cells&quot; #assign name , col_types = &quot;text&quot; #have to use text to preserve all data in single column ) ) pet_details %&gt;% knitr::kable() %&gt;% kableExtra::kable_styling(full_width = F, position = &quot;left&quot;) cells Tiddles 2 cat This is an improvement, we have a dataframe named pet_details comprising a single “cells” column, which contains all the relevant data from this worksheet. We could now try to reshape it, however, a better idea is to use map_dfc since we actually want to column bind these data rather than rowbind them. The read out from tibble::glimpse shows that the different variable types have been picked up, which is also helpful. The default naming of the columns gives a clue as to how the function works. pet_details &lt;- purrr::map_dfc( .x = c(&quot;B2&quot;, &quot;D5&quot;, &quot;E8&quot;) #vector of specific cells containing the data , ~ readxl::read_excel( path = &quot;data/pet_form_1.xlsx&quot; , range = .x , col_names = FALSE ) ) ## New names: ## New names: ## New names: ## New names: ## • `` -&gt; `...1` tibble::glimpse(pet_details) ## Rows: 1 ## Columns: 3 ## $ ...1 &lt;chr&gt; &quot;Tiddles&quot; ## $ ...2 &lt;dbl&gt; 2 ## $ ...3 &lt;chr&gt; &quot;cat&quot; …1 …2 …3 Tiddles 2 cat This is pretty close to what we want, the only sticking point is that we still don’t have the correct column names. We could deal with this using dplyr::rename, but an even better idea is to use purrr::map2_dfc. The map2 variant allows you to iterate over two arguments simultaneously (into the same function). pet_details_2 &lt;- purrr::map2_dfc( .x = c(&quot;B2&quot;, &quot;D5&quot;, &quot;E8&quot;) #vector of specific data cells , .y = c(&quot;Name&quot;, &quot;Age&quot;, &quot;Species&quot;) #vector of column names , ~ readxl::read_excel( path = &quot;data/pet_form_1.xlsx&quot; , range = .x , col_names = .y ) ) tibble::glimpse(pet_details_2) ## Rows: 1 ## Columns: 3 ## $ Name &lt;chr&gt; &quot;Tiddles&quot; ## $ Age &lt;dbl&gt; 2 ## $ Species &lt;chr&gt; &quot;cat&quot; Name Age Species Tiddles 2 cat 4.5.6 Non-rectangular data - single worksheet - many workbooks Having solved for one workbook and worksheet, we can functionalise and iterate to gather the data from every workbook, two of which are shown below. knitr::include_graphics(&quot;image/pet_forms.png&quot;) The function cells_to_rows below iterates over read_excel reading each of the three cells from the worksheet, applying the corresponding column name as it goes. It takes three character or character vector inputs, path, cells, and col_names. cells_to_rows &lt;- function(path, cells, col_names){ purrr::map2_dfc( .x = cells , .y = col_names , ~ readxl::read_excel( path = path , range = .x , col_names = .y ) ) } Let’s test it on the first pet form data, first setting the parameters to use in the function. path &lt;- &quot;data/pet_form_1.xlsx&quot; cells &lt;- c(&quot;B2&quot;, &quot;D5&quot;, &quot;E8&quot;) col_names &lt;- c(&quot;Name&quot;, &quot;Age&quot;, &quot;Species&quot;) pet_form_1 &lt;- cells_to_rows( path = path, cells = cells, col_names = col_names ) Name Age Species Tiddles 2 cat It works! So now we can iterate this over all the pet form workbooks, specifying the paths using regex as before. Note below we use .x in the path argument in the cells_to_rows function to refer to the vector of paths piped to purrr::map_dfr from fs::dir_ls. cells &lt;- c(&quot;B2&quot;, &quot;D5&quot;, &quot;E8&quot;) col_names &lt;- c(&quot;Name&quot;, &quot;Age&quot;, &quot;Species&quot;) all_pet_forms &lt;- fs::dir_ls( path = &quot;data&quot;, regex = &quot;pet_form_\\\\d\\\\.xlsx$&quot;) %&gt;% purrr::map_dfr( ~ cells_to_rows(path = .x, cells = cells, col_names = col_names) , .id = &quot;path&quot; ) path Name Age Species data/pet_form_1.xlsx Tiddles 2.0 cat data/pet_form_2.xlsx Woof 1.0 dog data/pet_form_3.xlsx Hammy 0.5 hamster data/pet_form_4.xlsx Toothless 3.0 dragon 4.5.7 Non-rectangular data - many worksheets - single workbook Now we have more than one worksheet in a single workbook, and the data looks like this, the workbook is from a “pet house” and each worksheet is pet details. knitr::include_graphics(&quot;image/pet_house.png&quot;) To incorporate the worksheets element we rejig the cells_to_rows function from above and give it a “sheet” argument, so it can be passed a specific sheet. sheet_cells_to_rows &lt;- function(path, sheet, cells, col_names){ purrr::map2_dfc( .x = cells , .y = col_names , ~ readxl::read_excel( path = path , sheet = sheet , range = .x , col_names = .y ) ) } We now have the function sheet_cells_to_rows that can accept a list of worksheet names. As before we use readxl::excel_sheets to collect the worksheet names, first setting the other parameters path &lt;- &quot;data/pet_house_1.xlsx&quot; cells &lt;- c(&quot;B2&quot;, &quot;D5&quot;, &quot;E8&quot;) col_names &lt;- c(&quot;Name&quot;, &quot;Age&quot;, &quot;Species&quot;) pet_house_1 &lt;- readxl::excel_sheets(path = path) %&gt;% purrr::set_names() %&gt;% purrr::map_dfr( ~ sheet_cells_to_rows(path = path , sheet = .x , cells = cells , col_names = col_names) , .id = &quot;sheet&quot; ) 4.5.8 Non-rectangular data - many worksheets - many workbooks Finally we have many workbooks each containing many worksheets, each containing many cells, as before we want to read them in and combine. knitr::include_graphics(&quot;image/pet_houses.png&quot;) We could functionalise the code above that reads and combines the cells in many worksheets from a single workbook, but an alternative approach is used below. We create an anonymous function and use that on the fly. This is useful if the function is a one off, and not too complicated. The anonymous function below still depends on the sheet_cells_to_rows we created earlier though. cells &lt;- c(&quot;B2&quot;, &quot;D5&quot;, &quot;E8&quot;) col_names &lt;- c(&quot;Name&quot;, &quot;Age&quot;, &quot;Species&quot;) pet_house_all &lt;- fs::dir_ls( path = &quot;data&quot;, regex = &quot;pet_house_\\\\d\\\\.xlsx$&quot;) %&gt;% purrr::map_dfr( function(path){ readxl::excel_sheets(path = path) %&gt;% purrr::set_names() %&gt;% purrr::map_dfr( ~ sheet_cells_to_rows(path = path , sheet = .x , cells = cells , col_names = col_names) , .id = &quot;sheet&quot; ) } , .id = &quot;path&quot; ) path sheet Name Age Species data/pet_house_1.xlsx pet_1 Tiddles 2.0 cat data/pet_house_1.xlsx pet_2 Leafy 0.8 shield bug data/pet_house_2.xlsx pet_1 Tuuli 8.0 tiger data/pet_house_2.xlsx pet_2 Sophie 4.0 cheetah data/pet_house_2.xlsx pet_3 Pearl 3.0 Chilean rose tarantula 4.6 Exporting to .xlsx We recommend {openxlsx} for writing and formatting tables to MS Excel. openxlsx manual 4.7 .sav Use {haven} to import SPSS, Stata and SAS files. 4.8 SQL Below are links to DfT Coffee and Coding materials on the subject of connecting R to SQL 20181114_Connecting_R_to_SQL 20190130_SQL_and_Excel_to_R 4.9 GCP 4.9.1 BigQuery Digital guidance on how to use {bigrquery} to interact with GCP’s BigQuery 4.9.2 Google Cloud Storage Digital guidance on how to connect to GCS using R. "],["tables.html", "Chapter 5 Table/Data Frame manipulation 5.1 Pivoting tables 5.2 Dropping and selecting columns 5.3 Rename variables 5.4 Filtering data 5.5 Group data 5.6 Order data 5.7 Get counts of data 5.8 Combine tables 5.9 Joining tables 5.10 Select specific columns in a join 5.11 Sum rows or columns 5.12 Replace NAs or other values 5.13 Reordering rows/columns 5.14 Creating new variables 5.15 Summarising data 5.16 Look up tables", " Chapter 5 Table/Data Frame manipulation This chapter provides an overview of code examples for table or data frame manipulation (a tidyverse data frame is referred to as a tibble). One of the main things you will have to do in any R project or RAP project will be manipulating the data that you are using in order to get it into the format you require. One of the main packages used to manipulate data is the {dplyr} package which we recommend and use throughout this book. The {dplyr} package (and others e.g. {tidyr}) are all part of the tidyverse. The tidyverse is a group of packages developed by Hadley Wickham and others and are all designed to work with each other. See https://www.tidyverse.org/ for more info. Tidyverse packages and functions can be combined and layered using the pipe operator %&gt;%. {dplyr} is built to work with tidy data. To find out more about tidy data please look at the following link https://r4ds.had.co.nz/tidy-data.html but the general principles are: Each variables must have its own column Each observation must have its own row Each value must have its own cell 5.1 Pivoting tables There are two packages still in common use for pivoting data into tidy format. The {tidyr} package is a tidyverse package which contains functions used to create tidy data (among other data cleaning functions) The {reshape2} package is the non-tidyverse equivalent used to pivot tables. No updates have been made to this package in the last few years, and where possible tidyr should always be used in preference to it. We want to have the day of the week variable running along the top so each day of the week is its own column. Table 5.1: Number of road accidents by accident severity and weekday Accident_Severity Day_of_Week n 1 1 300 1 2 205 1 3 187 1 4 233 1 5 220 1 6 250 {tidyr} package The {tidyr} package is a tidyverse package for manipulating data into a tidy format, and should be the first choice for gathering and pivoting functions. Use of the tidyr package is made slightly more complicated by the fact that there are both legacy (prior to v1.2) and more recent (v1.2 onwards) pivoting verbs available: gather and spread The functions gather and spread were the tidyr functions used prior to version 1.2 of the package. These verbs are still supported in more recent versions of the package so old code will still work, but you should aim not to use these in new code. gather makes wide data longer: equivalent is now pivot_longer() spread makes long data wider: equivalent is now pivot_wider() pivot_longer and pivot_wider The pivot_longer and pivot_wider functions are the updated version of gather and spread, available in newer versions of the tidyr package. These are designed to have more intuative names, and more logical arguments in comparison to the older versions. You should aim to use these in all new code you are writing. pivot_longer makes wide data longer i.e. variables running along the top can be “gathered” into rows running down. pivot_wider makes long data wider i.e. one variable can be spread and run along the top with each value being a variable. # Pivot table using newer tidyr package road_accidents_weekdays &lt;- road_accidents_small %&gt;% tidyr::pivot_wider(names_from = Day_of_Week, values_from = n) In the pivot_wider function, you need to specify the dataset you want to transform, as well as the column you want to take the names from and the one you want to take the values from, in this case Day_of_Week and n respectively. Table 5.2: Number of road accidents by accident severity and weekday, tidyr::pivot_wider Accident_Severity 1 2 3 4 5 6 7 1 300 205 187 233 220 250 281 2 3009 2948 3230 3227 3246 3649 3225 3 11668 14783 16065 15859 16331 17346 13720 The opposite can also be done using the pivot_longer function: # Pivot table using newer tidyr package road_accidents_gather &lt;- road_accidents_weekdays %&gt;% tidyr::pivot_longer(cols = c(`1`, `2`, `3`, `4`, `5`, `6`, `7`), names_to = &quot;weekday&quot;, values_to = &quot;n&quot;) To use pivot_longer, specify which columns you want to be gathered into one column (in this case the individual weekday columns). Then specify the column name you’d like to move the names to (weekday) and the one you’d like to move the values to (n). It is also possible to pivot all columns except the ones you choose to exclude by specifying only those you do not want to pivot, using - to indicate this. In the example below, this is done by excluding the Accident_Severity column and pivoting all other columns by specifying cols = -Accident_Severity. This is useful when you may have a large number of columns or columns with unknown names that you want to turn into tidy data. # Pivot by excluding the accident severity column road_accidents_gather &lt;- road_accidents_weekdays %&gt;% tidyr::pivot_longer(cols = -Accident_Severity, names_to = &quot;weekday&quot;, values_to = &quot;n&quot;) Table 5.3: Number of road accidents by accident severity and weekday, tidyr::pivot_longer Accident_Severity weekday n 1 1 300 1 2 205 1 3 187 1 4 233 1 5 220 1 6 250 Extended data transpose example This section shows how to transpose data using the pivot_wider and pivot_longer functions with the tidyr package. The aim is to take a published table of reported road accidents in which each row contains the data for one year with the severity of accident recorded in the columns. The tranposition will enable the calculation of the percentage increases for each of the last two years. Firstly, the current year is specified. Yr &lt;- 2018 ## select for the latest of the three years you require This example uses data from RAS30013 “Reported casualty rates by road user type and severty: Great Britain”. https://www.gov.uk/government/statistical-data-sets/ras30-reported-casualties-in-road-accidents#casualty-rates-and-risk The contents of the published table is read in (the last 10 years only), and the adjusted data is rounded to the nearest whole number. Accidents &lt;- readxl::read_excel(&quot;data/ras10013.xlsx&quot;) %&gt;% dplyr::filter(Year &gt;= 2009) %&gt;% dplyr::mutate(SeriousAdjusted = round(SeriousAdjusted), FatalSeriousAdjusted = round(FatalSeriousAdjusted), SlightAdjusted = round(SlightAdjusted)) Year Fatal SeriousUnadjusted SeriousAdjusted FatalSeriousUnadjusted FatalSeriousAdjusted SlightUnadjusted SlightAdjusted AllAccidents 2,009 2,057 21,997 34,198 24,054 36,255 139,500 127,299 163,554 2,010 1,731 20,440 31,831 22,171 33,562 132,243 120,852 154,414 2,011 1,797 20,986 32,476 22,783 34,273 128,691 117,201 151,474 2,012 1,637 20,901 32,005 22,538 33,642 123,033 111,929 145,571 2,013 1,608 19,624 30,243 21,232 31,851 117,428 106,809 138,660 2,014 1,658 20,676 31,889 22,334 33,547 123,988 112,775 146,322 2,015 1,616 20,038 30,474 21,654 32,090 118,402 107,966 140,056 2,016 1,695 21,725 29,072 23,420 30,767 113,201 105,854 136,621 2,017 1,676 22,534 27,938 24,210 29,614 105,772 100,368 129,982 2,018 1,671 23,165 28,266 24,836 29,937 97,799 92,698 122,635 2,019 1,658 23,422 27,222 25,080 28,880 92,456 88,656 117,536 The data is now filtered to keep only the last three years of data. We could have hard coded the year in the filter below e.g. Year %in% c(2017,2018,2019). To save having to change the code every time we wish to run for different years, we can derive the required years from a previously defined variable. This picks up the most recent year from yr defined above and calculates the other years as 1 and 2 years before yr as yr-1 and yr-2. A new variable YearLab with the respective text ThisYear, LastYear and YearBefore is created to automate calculations below. Last3Years &lt;- dplyr::filter(Accidents, Year %in% c(Yr, Yr-1, Yr-2)) %&gt;% dplyr::mutate(YearLab = dplyr::case_when( Year == Yr ~ &quot;ThisYear&quot;, Year == Yr-1 ~ &quot;LastYear&quot;, TRUE ~ &quot;YearBefore&quot; # else option )) %&gt;% dplyr::select(-Year) Fatal SeriousUnadjusted SeriousAdjusted FatalSeriousUnadjusted FatalSeriousAdjusted SlightUnadjusted SlightAdjusted AllAccidents YearLab 1,695 21,725 29,072 23,420 30,767 113,201 105,854 136,621 YearBefore 1,676 22,534 27,938 24,210 29,614 105,772 100,368 129,982 LastYear 1,671 23,165 28,266 24,836 29,937 97,799 92,698 122,635 ThisYear The table above holds the number of accidents for each severity (in columns) with a row for each of the last three years. We wish to calculate the annual growth in accidents by severity. To achieve this we need to transpose the table. This can be achieved using the pivot_longer function within the tidyr package which allows us to put data into a tidy format which makes it easier to manipulate. The template of the pivot_longer function is: tidyr::pivot_longer( cols = variables_to_input_into_one_column, names_to = column to move data names to, values_to = column to move data values to) The variables we want to input into one column are the accident severities (Fatal, SeriousUnadjusted, …, AllAccidents); every column except for the YearLab. The names_to parameter we will call SeverityType. The values_to parameter is the name of the column holding the number of accidents (n). The variable YearLabel is the only one that will be unchanged, so can be specifically excluded from the pivot_longer call. A new field SeverityNum is created so that the severity types can be sorted in a logical order in the final output. Severity &lt;- Last3Years %&gt;% tidyr::pivot_longer(cols = -YearLab, names_to = &quot;SeverityType&quot;, values_to = &quot;n&quot;) %&gt;% dplyr::mutate(SeverityNum = dplyr::case_when( SeverityType==&#39;Fatal&#39; ~ 1, SeverityType==&#39;SeriousUnadjusted&#39; ~ 2, SeverityType==&#39;SeriousAdjusted&#39; ~ 3, SeverityType==&#39;FatalSeriousUnadjusted&#39; ~ 4, SeverityType==&#39;FatalSeriousAdjusted&#39; ~ 5, SeverityType==&#39;SlightUnadjusted&#39; ~ 6, SeverityType==&#39;SlightAdjusted&#39; ~ 7, SeverityType==&#39;AllAccidents&#39; ~ 8, TRUE ~ 0 # else option )) YearLab SeverityType n SeverityNum YearBefore Fatal 1,695 1 YearBefore SeriousUnadjusted 21,725 2 YearBefore SeriousAdjusted 29,072 3 YearBefore FatalSeriousUnadjusted 23,420 4 YearBefore FatalSeriousAdjusted 30,767 5 YearBefore SlightUnadjusted 113,201 6 YearBefore SlightAdjusted 105,854 7 YearBefore AllAccidents 136,621 8 LastYear Fatal 1,676 1 LastYear SeriousUnadjusted 22,534 2 LastYear SeriousAdjusted 27,938 3 LastYear FatalSeriousUnadjusted 24,210 4 LastYear FatalSeriousAdjusted 29,614 5 LastYear SlightUnadjusted 105,772 6 LastYear SlightAdjusted 100,368 7 LastYear AllAccidents 129,982 8 ThisYear Fatal 1,671 1 ThisYear SeriousUnadjusted 23,165 2 ThisYear SeriousAdjusted 28,266 3 ThisYear FatalSeriousUnadjusted 24,836 4 ThisYear FatalSeriousAdjusted 29,937 5 ThisYear SlightUnadjusted 97,799 6 ThisYear SlightAdjusted 92,698 7 ThisYear AllAccidents 122,635 8 We now wish to transpose this table so that the three years (ThisYear, LastYear and YearBefore) will be in separate columns rather than one. To do this, we use the pivot_wider function within the tidyr package. The template of the pivot_wider function is: tidyr::pivot_wider(cols = variables_to_change_from rows_into_columns, names_from = column_containing_variable_names, values_form = column_containing_values) Here the column containing the names is YearLab, and the column containing the values is the number of accidents n. The variables SeverityType and SeverityNum will each remain in one column so are not listed in the pivot_wider function. By default, the data would be sorted in alphabetical order of SeverityType. Instead the data is sorted in a pre-defined logical order using the SeverityNum variable defined above. The SeverityNum, having done its job, is then dropped from the final output. Now that the number of accidents for each year are in a separate column, the percentage growth for one year to the next can be calculated using the mutate function. Transposed &lt;- Severity %&gt;% tidyr::pivot_wider(names_from = YearLab, values_from = n) %&gt;% dplyr::arrange(SeverityNum) %&gt;% dplyr::select(-SeverityNum) %&gt;% dplyr::mutate(PerGrowthThisYear = round(ThisYear/LastYear *100-100,1), PerGrowthLastYear=round(LastYear/YearBefore *100-100,1)) SeverityType YearBefore LastYear ThisYear PerGrowthThisYear PerGrowthLastYear Fatal 1,695 1,676 1,671 -0.3 -1.1 SeriousUnadjusted 21,725 22,534 23,165 2.8 3.7 SeriousAdjusted 29,072 27,938 28,266 1.2 -3.9 FatalSeriousUnadjusted 23,420 24,210 24,836 2.6 3.4 FatalSeriousAdjusted 30,767 29,614 29,937 1.1 -3.7 SlightUnadjusted 113,201 105,772 97,799 -7.5 -6.6 SlightAdjusted 105,854 100,368 92,698 -7.6 -5.2 AllAccidents 136,621 129,982 122,635 -5.7 -4.9 5.2 Dropping and selecting columns Use the {dplyr} select function to both select and drop columns. Select columns road_accidents_4_cols &lt;- road_accidents %&gt;% dplyr::select(acc_index, Accident_Severity, Date, Police_Force) Table 5.4: Four columns from road accidents 2017 acc_index Accident_Severity Date Police_Force 2017010001708 1 2017-08-05 1 2017010009342 3 2017-01-01 1 2017010009344 3 2017-01-01 1 2017010009348 3 2017-01-01 1 2017010009350 2 2017-01-01 1 2017010009351 3 2017-01-01 1 Drop columns As for pivoting, to drop columns the difference is putting a “-” in front of the variable name. road_accidents_3_cols &lt;- road_accidents_4_cols %&gt;% dplyr::select(-Police_Force) Table 5.5: Three columns from road accidents 2017 acc_index Accident_Severity Date 2017010001708 1 2017-08-05 2017010009342 3 2017-01-01 2017010009344 3 2017-01-01 2017010009348 3 2017-01-01 2017010009350 2 2017-01-01 2017010009351 3 2017-01-01 5.3 Rename variables Use the rename function from {dplyr} to rename variables where the new variable name is on the left hand side of the = equals sign, and the old variable name is on the right hand. road_accidents_rename &lt;- road_accidents_4_cols %&gt;% dplyr::rename(Date_of_Accident = Date) Table 5.6: Rename date column to Date_of_Accident acc_index Accident_Severity Date_of_Accident Police_Force 2017010001708 1 2017-08-05 1 2017010009342 3 2017-01-01 1 2017010009344 3 2017-01-01 1 2017010009348 3 2017-01-01 1 2017010009350 2 2017-01-01 1 2017010009351 3 2017-01-01 1 5.4 Filtering data Use the {dplyr} filter function to filter data. This example filters the data for slight severity accidents (accident severity 3). road_accidents_slight &lt;- road_accidents_4_cols %&gt;% dplyr::filter(Accident_Severity == 3) Table 5.7: Slight severity road accidents 2017 acc_index Accident_Severity Date Police_Force 2017010009342 3 2017-01-01 1 2017010009344 3 2017-01-01 1 2017010009348 3 2017-01-01 1 2017010009351 3 2017-01-01 1 2017010009353 3 2017-01-01 1 2017010009354 3 2017-01-01 1 To filter multiple conditions: And operator road_accidents_filter &lt;- road_accidents_4_cols %&gt;% dplyr::filter(Accident_Severity == 3 &amp; Police_Force == 4) Or operator road_accidents_filter2 &lt;- road_accidents_4_cols %&gt;% dplyr::filter(Accident_Severity == 3 | Accident_Severity == 2) Note: filtering with characters must be wrapped in “quotation marks” e.g: road_accidents_filter3 &lt;- road_accidents %&gt;% dplyr::filter(`Local_Authority_(Highway)` == &quot;E09000010&quot;) Also note that in the above example the variable is quoted in back ticks (`). This is necessary when a column or variable name contains non-standard characters (anything other than alphanumerics, and selected punctuation marks: _, - or .) or start with numbers; they need to be wrapped in back ticks so R knows that everything inside the back ticks is a variable name. 5.5 Group data Use the {dplyr} group_by function to group data. This works in a similar manner to “GROUP BY” in SQL. The below example groups the data by accident severity and weekday, and creates totals for each group using the “tally” function. # Create grouped data set with counts road_accidents_small &lt;- road_accidents %&gt;% dplyr::group_by(Accident_Severity, Day_of_Week) %&gt;% dplyr::tally() Table 5.8: Road accidents 2017 by accident severity and weekday Accident_Severity Day_of_Week n 1 1 300 1 2 205 1 3 187 1 4 233 1 5 220 1 6 250 5.6 Order data Use the {dplyr} arrange function to order data. This works in a similar manner to “ORDER BY” in SQL. This example orders the data by date and number of casualties. # Order data by date and number of casualties road_accidents_ordered &lt;- road_accidents %&gt;% dplyr::sample_n(7) %&gt;% dplyr::select(acc_index, Accident_Severity, Police_Force, Number_of_Casualties, Date) %&gt;% dplyr::arrange(Date, Number_of_Casualties) Table 5.9: Road accidents 2017 ordered by date and number of casualties acc_index Accident_Severity Police_Force Number_of_Casualties Date 201797UA00802 3 97 1 2017-02-09 2017010036859 2 1 1 2017-05-11 2017134791665 3 13 1 2017-07-09 2017200259717 3 20 1 2017-12-01 2017010076218 3 1 2 2017-12-03 2017010077687 3 1 1 2017-12-13 5.7 Get counts of data To get counts for groups of data, the {dplyr} tally function can be used in conjunction with the {dplyr} group by function. This groups the data into the required groups and then tallys how many records are in each group. # Create grouped data set with counts road_accidents_small &lt;- road_accidents %&gt;% dplyr::group_by(Accident_Severity, Day_of_Week) %&gt;% dplyr::tally() The above example creates groups by accident severity and weekday and counts how many accidents are in each group (one row equals one accident therefore the tally is counting accidents). Table 5.10: Road accidents 2017 by accident severity and weekday Accident_Severity Day_of_Week n 1 1 300 1 2 205 1 3 187 1 4 233 1 5 220 1 6 250 5.8 Combine tables When combining data from two tables there are two ways to do this in R: Bind the tables by basically either appending the tables on the rows or columns Join the tables using the {dplyr} version of SQL joins Binding tables Binding tables is mainly done to append tables by creating more rows, however tables can also be bound by adding more columns. Although it is recommended to use the {dplyr} join functions to combine columns (see 5.6). Here are three tables, one shows data for accident severity of 1, one for accident severity of 2, and one for accident severity of 3. Table 5.11: Number of fatal road accidents in 2017, by weekday Accident_Severity Day_of_Week n 1 1 300 1 2 205 1 3 187 1 4 233 1 5 220 1 6 250 1 7 281 Table 5.11: Number of serious injury road accidents in 2017, by weekday Accident_Severity Day_of_Week n 2 1 3009 2 2 2948 2 3 3230 2 4 3227 2 5 3246 2 6 3649 2 7 3225 Table 5.11: Number of slight injury road accidents in 2017, by weekday Accident_Severity Day_of_Week n 3 1 11668 3 2 14783 3 3 16065 3 4 15859 3 5 16331 3 6 17346 3 7 13720 To combine these tables we can use the bind_rows function from the {dplyr} package. Use bind_rows when you want to append the tables underneath one another to make one longer table, i.e. you want to add more rows. Ensure that the column names for each table are exactly the same in each table. # combine tables using bind_rows library(dplyr) all_accidents &lt;- accidents_1 %&gt;% dplyr::bind_rows(accidents_2, accidents_3) Table 5.12: Road accident data 2017, bind_rows Accident_Severity Day_of_Week n 1 1 300 1 2 205 1 3 187 1 4 233 1 5 220 1 6 250 1 7 281 2 1 3009 2 2 2948 2 3 3230 2 4 3227 2 5 3246 2 6 3649 2 7 3225 3 1 11668 3 2 14783 3 3 16065 3 4 15859 3 5 16331 3 6 17346 3 7 13720 5.9 Joining tables Joins in R can be done using {dplyr}. This is generally to combine columns of data from two tables: # combine tables using left join library(dplyr) all_accidents_cols_join &lt;- road_acc_1 %&gt;% dplyr::left_join(road_acc_2, by = &quot;acc_index&quot;) This uses the same principles as SQL, by specifying what the tables should be joined on using the by = argument. {dplyr} has all the usual SQL joins for example, inner_join, full_join, right_join. All of these are used in the same way as the left join example above. Another useful join for data manipulation is an anti_join. This provides all the data that is not in the joined table. For example, the below snapshot of a table displays road accident totals broken down by accident severity and weekday: Accident_Severity Day_of_Week n 1 1 300 1 2 205 1 3 187 1 4 233 1 5 220 1 6 250 I am interested in creating two sub-groups of this data, a table for all accidents on a Monday (weekday 2), and all other accidents. First, I get the Monday data using the {dplyr} filter function (see 5.3). Accident_Severity Day_of_Week n 1 2 205 2 2 2948 3 2 14783 Then, I can use an anti-join to create a table which has all of the data that is not in the above table: # create table of all rows not in the joined table library(dplyr) all_accidents_not_monday &lt;- road_accidents_small %&gt;% dplyr::anti_join(accidents_monday, by = c(&quot;Accident_Severity&quot;, &quot;Day_of_Week&quot;)) The above code takes the initial table we want to get our data from (road_accidents_small) and anti joins accidents_monday. This says, “get all the rows from road_accidents_small that are not in accidents_monday”. Again, note the need to specify what the join rows would be joined and compared by. Table 5.13: Road accident data 2017 not on a Monday by accident severity Accident_Severity Day_of_Week n 1 1 300 1 3 187 1 4 233 1 5 220 1 6 250 1 7 281 2 1 3009 2 3 3230 2 4 3227 2 5 3246 2 6 3649 2 7 3225 3 1 11668 3 3 16065 3 4 15859 3 5 16331 3 6 17346 3 7 13720 5.10 Select specific columns in a join Doing a join with {dplyr} will join all columns from both tables, however sometimes not all columns from each table are needed. Let’s look at some previous tables again: Table 5.14: Police force and accident severity information for accidents acc_index Police_Force Accident_Severity 2017010001708 1 1 2017010009342 1 3 2017010009344 1 3 2017010009348 1 3 2017010009350 1 2 2017010009351 1 3 Table 5.14: Date and weekday information for accidents acc_index Date Day_of_Week 2017010001708 2017-08-05 7 2017010009342 2017-01-01 1 2017010009344 2017-01-01 1 2017010009348 2017-01-01 1 2017010009350 2017-01-01 1 2017010009351 2017-01-01 1 Let’s say we want acc_index and Police_Force from the first table, and Date from the second table. # select specific columns from each table and left join library(dplyr) road_acc_3 &lt;- road_acc_1 %&gt;% dplyr::select(acc_index, Police_Force) %&gt;% dplyr::left_join(select(road_acc_2, acc_index, Date), by = &quot;acc_index&quot;) The above code takes the first table and uses the select statement to select the required columns from the first table. Then within the left_join command, to select the data from the second table, you again add the select statement. Note: you will need to select the joining variable in both tables but this will only appear once Table 5.15: Police force and Date information for specific accidents acc_index Police_Force Date 2017010001708 1 2017-08-05 2017010009342 1 2017-01-01 2017010009344 1 2017-01-01 2017010009348 1 2017-01-01 2017010009350 1 2017-01-01 2017010009351 1 2017-01-01 5.11 Sum rows or columns 5.11.1 Sum rows Summing across a row can be done using the dplyr function c_across; you just need to specify that this should be done row-wise before performing the calculation: # sum across a row road_accidents_weekdays &lt;- road_accidents_weekdays %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(rowsum = sum(c_across())) Table 5.16: Road accidents 2017 by accident severity and weekday Accident_Severity 1 2 3 4 5 6 7 rowsum 1 300 205 187 233 220 250 281 1676 2 3009 2948 3230 3227 3246 3649 3225 22534 3 11668 14783 16065 15859 16331 17346 13720 105772 To sum across specific rows, you can name these within the c_across function: # sum across specific rows road_accidents_weekdays &lt;- road_accidents_weekdays %&gt;% dplyr::rowwise() %&gt;% dplyr::mutate(alldays = sum(c_across(`1`:`5`))) Table 5.17: Road accidents 2017 by accident severity and weekday Accident_Severity 1 2 3 4 5 6 7 alldays 1 300 205 187 233 220 250 281 1145 2 3009 2948 3230 3227 3246 3649 3225 15660 3 11668 14783 16065 15859 16331 17346 13720 74706 5.11.2 Sum columns Columns can also be summed, although it isn’t recommended that these sums are added to a data table: # sum columns road_accidents_weekdays %&gt;% dplyr::summarise_if(is.numeric, sum) 5.12 Replace NAs or other values To replace all NAs in one column (Junction Control column) with a specific value: library (tidyr) # replace all NAs with value -1 road_accidents_na &lt;- road_accidents_na %&gt;% dplyr::mutate(Junction_Control = tidyr::replace_na(Junction_Control, -1)) Note: To replace NA with a character the character replacement must be wrapped in “quotation marks” To replace all NAs in a data frame or tibble: # replace all NAs with value -1 road_accidents_na &lt;- road_accidents_na %&gt;% mutate_if(is.numeric, tidyr::replace_na, -1) To replace values with NA, specify what value you want to be replaced with NA using the na_if function: # create nas road_accidents_na &lt;- road_accidents_na %&gt;% dplyr::na_if(-1) Note: to only create NAs in a specific column specify the column name in a similar manner to the first example in this section. To replace values: # replace 1st_road_class road_accidents_na &lt;- road_accidents_na %&gt;% dplyr::mutate(`1st_Road_Class` = dplyr::case_when(`1st_Road_Class` == 3 ~ &quot;A Road&quot;, TRUE ~ as.character(`1st_Road_Class`))) The case_when function is similar to using CASE WHEN in SQL. The TRUE argument indicates that if the values aren’t included in the case_when then they should be whatever is after the tilda (~) i.e. the equivalent of the ELSE statement in SQL. The “as.character” function says that everything that in 1st_Road_Class isn’t 3 should be kept as it is, this could be replaced by an arbitrary character or value e.g. “Other”. This would make everything that is not a 3, coded as “Other”. You can have multiple case_when arguments for multiple values, they just need to be seperated with a comma. Multiple case_when statements for different variables can be layered using the pipe operator %&gt;%. 5.13 Reordering rows/columns 5.13.1 Reordering rows Rows can be reordered by certain variables using the {dplyr} arrange function with examples in the 4.5 Order data sub-chapter of this book. This will order the data in ascending order by the variables quoted. To order rows in descending order the desc() command can be used within the arrange function. # Order data by date and number of casualties road_accidents_ordered_desc &lt;- road_accidents %&gt;% dplyr::select(acc_index, Accident_Severity, Police_Force, Number_of_Casualties, Date) %&gt;% dplyr::arrange(desc(Date), Number_of_Casualties) Table 5.18: Road accidents 2017 ordered by date (descending) and number of casualties acc_index Accident_Severity Police_Force Number_of_Casualties Date 2017010065745 2 1 1 2017-12-31 2017010080880 3 1 1 2017-12-31 2017010080888 3 1 1 2017-12-31 2017010080889 2 1 1 2017-12-31 2017010080894 3 1 1 2017-12-31 2017010080901 3 1 1 2017-12-31 5.13.2 Reordering columns Use the {dplyr} select statement to reorder columns, where the order of the variables quoted represents the order of the columns in the table. Table 5.19: Four columns from road accidents 2017 acc_index Accident_Severity Date Police_Force 2017010001708 1 2017-08-05 1 2017010009342 3 2017-01-01 1 2017010009344 3 2017-01-01 1 2017010009348 3 2017-01-01 1 2017010009350 2 2017-01-01 1 2017010009351 3 2017-01-01 1 To reorder this table we do: table_reordered &lt;- road_accidents_4_cols %&gt;% dplyr::select(Accident_Severity, Date, acc_index, Police_Force) 5.14 Creating new variables The {dplyr} mutate function can be used to create new variables based on current variables or other additional information. For example, to create a new variable which is speed limit in km: road_acc_km &lt;- road_accidents %&gt;% dplyr::mutate(speed_km = Speed_limit * 1.6) %&gt;% dplyr::select(acc_index, Police_Force, Speed_limit, speed_km) Table 5.20: Road accidents by km/h acc_index Police_Force Speed_limit speed_km 2017010001708 1 30 48 2017010009342 1 30 48 2017010009344 1 30 48 2017010009348 1 30 48 2017010009350 1 20 32 2017010009351 1 30 48 5.15 Summarising data The {dplyr} summarise function can be used to summarise data (mean, median, sd, min, max, n_distinct). See https://dplyr.tidyverse.org/reference/summarise.html for more examples. For example, to get the mean number of accidents for each weekday: Table 5.21: Road accidents 2017, by severity and weekday Accident_Severity Day_of_Week n 1 1 300 1 2 205 1 3 187 1 4 233 1 5 220 1 6 250 The group by function is used with the summarise function to specify what groups the mean will be applied to, in this case weekday. road_acc_mean &lt;- road_accidents_small %&gt;% dplyr::group_by(Day_of_Week) %&gt;% dplyr::summarise(mean = mean(n)) Table 5.22: Mean number of accidents in 2017, by weekday Day_of_Week mean 1 4992.333 2 5978.667 3 6494.000 4 6439.667 5 6599.000 6 7081.667 5.16 Look up tables Aside from importing a separate lookup data file into R, named vectors can be used as lookup tables. For example, to assign accident severity values with labels, named vectors can be used (note: numbers must also be in quotation marks): lookup_severity &lt;- c(&quot;1&quot; = &quot;Fatal&quot;, &quot;2&quot; = &quot;Serious&quot;, &quot;3&quot; = &quot;Slight&quot;) To convert the data and create a label variable (note: the Accident_Severity variable values can be replaced with the lookup values by changing the name of the variable on the left to Accident_Severity): road_accidents_small$Accident_Severity_label &lt;- lookup_severity[road_accidents_small$Accident_Severity] Table 5.23: Road accidents 2017, by severity and weekday Accident_Severity Day_of_Week n Accident_Severity_label 1 1 300 Fatal 1 2 205 Fatal 1 3 187 Fatal 1 4 233 Fatal 1 5 220 Fatal 1 6 250 Fatal "],["dates-times.html", "Chapter 6 Working with dates and times 6.1 Working with dates 6.2 Working with date-times", " Chapter 6 Working with dates and times This chapter provides an overview of working with dates and times, for example extracting year or month from a date, and converting characters to a date. One of the main packages used to work with dates is {lubridate}. More information can be found on the {lubridate} cheatsheet at the following link: https://www.rstudio.com/resources/cheatsheets/ Date vectors are just vectors of class double with an additional class attribute set as “Date”. DfT_birthday &lt;- lubridate::as_date(&quot;1919-08-14&quot;) typeof(DfT_birthday) #&gt; [1] &quot;double&quot; attributes(DfT_birthday) #&gt; $class #&gt; [1] &quot;Date&quot; If we remove the class using unclass() we can reveal the value of the double, which is the number of days since “1970-01-01”3, since DfT’s birthday is before this date, the double is negative. unclass(DfT_birthday) #&gt; [1] -18403 This chapter will be using the road accident data set: Table 6.1: Reported Road Accidents, 2017 acc_index Date Police_Force Day_of_Week Time 2017010001708 2017-08-05 1 7 1899-12-31 03:12:00 2017010009342 2017-01-01 1 1 1899-12-31 01:30:00 2017010009344 2017-01-01 1 1 1899-12-31 00:30:00 2017010009348 2017-01-01 1 1 1899-12-31 01:11:00 2017010009350 2017-01-01 1 1 1899-12-31 01:42:00 2017010009351 2017-01-01 1 1 1899-12-31 03:31:00 6.1 Working with dates 6.1.1 Converting a character to a date In R, dates can be converted to a specific date variable type in order to use the variable as a date. Having a variable as a date means that you can: extract the different elements of the date (year, month etc.) calculate differences between dates This can be done in the following way: Identify the order of the year, month, day and use the appropriate function (ymd, mdy, dmy etc.) # convert date to date object # check class of date class(road_accidents$Date1) #&gt; [1] &quot;character&quot; # look at the date variable and see what order it is in (year-m-d) # therefore use the ymd function road_accidents$Date1 &lt;- lubridate::ymd(road_accidents$Date1) # now check class class(road_accidents$Date1) #&gt; [1] &quot;Date&quot; 6.1.2 Get year from date Use the year function from {lubridate}: road_accidents$Year &lt;- lubridate::year(road_accidents$Date1) See Table 5.2 for output 6.1.3 Get month from date Use the month function from {lubridate}: road_accidents$Month &lt;- lubridate::month(road_accidents$Date1) See Table 5.2 for output 6.1.4 Get day from date Use the day function from {lubridate}: road_accidents$Day &lt;- lubridate::day(road_accidents$Date1) See Table 5.2 for output 6.1.5 Get weekday from date Use the wday function from {lubridate} to get the weekday label: road_accidents$weekday &lt;- lubridate::wday(road_accidents$Date1) See Table 5.2 for output 6.1.6 Get quarter from date Use the quarter function from {lubridate}: road_accidents$Quarter &lt;- lubridate::quarter(road_accidents$Date1) See Table 5.2 for output Table 6.2: Using lubridate to extract time information Date1 Year Quarter Month Day weekday 2017-08-05 2017 3 8 5 7 2017-01-01 2017 1 1 1 1 2017-01-01 2017 1 1 1 1 2017-01-01 2017 1 1 1 1 2017-01-01 2017 1 1 1 1 2017-01-01 2017 1 1 1 1 6.1.7 Find difference between two dates Table 6.3: Find difference between two dates acc_index Date1 Date2 2017010001708 2017-08-05 2017-08-01 2017010009342 2017-01-01 2017-01-01 2017010009344 2017-01-01 2017-01-01 2017010009348 2017-01-01 2017-01-01 2017010009350 2017-01-01 2017-01-01 2017010009351 2017-01-01 2017-01-01 Use the as.duration function to find the duration between two dates. The duration to be measured can be specified: dhours dweeks ddays dminutes dyears To find out the number of days difference, the as.duration function calculates the duration in seconds so the duration must be divided by the desired duration (ddays) to convert to duration in days. road_accidents$date_diff &lt;- lubridate::as.duration(road_accidents$Date2 %--% road_accidents$Date1) / ddays(1) Table 6.4: Find difference between two dates acc_index Date1 Date2 date_diff 2017010001708 2017-08-05 2017-08-01 4 2017010009342 2017-01-01 2017-01-01 0 2017010009344 2017-01-01 2017-01-01 0 2017010009348 2017-01-01 2017-01-01 0 2017010009350 2017-01-01 2017-01-01 0 2017010009351 2017-01-01 2017-01-01 0 The %--% operator is used to define an interval. So, this code is calculating the duration of the interval between Date2 and Date1. The number after ddays indicates by how many units the duration is (i.e. one day). 6.1.8 Convert month (integer to character) {base} R has a useful function which takes the month numbers and converts them to the corresponding text. road_accidents$Month_lab &lt;- month.abb[road_accidents$Month] Table 6.5: Convert month to character acc_index Date Month Month_lab 2017010001708 2017-08-05 8 Aug 2017010009342 2017-01-01 1 Jan 2017010009344 2017-01-01 1 Jan 2017010009348 2017-01-01 1 Jan 2017010009350 2017-01-01 1 Jan 2017010009351 2017-01-01 1 Jan 6.1.9 Convert month (character to integer) {base} R has a useful function which takes the month text and converts them to the corresponding number. road_accidents$Month &lt;- match(road_accidents$Month_lab,month.abb) Table 6.6: Convert month character to integer acc_index Date Month Month_lab 2017010001708 2017-08-05 8 Aug 2017010009342 2017-01-01 1 Jan 2017010009344 2017-01-01 1 Jan 2017010009348 2017-01-01 1 Jan 2017010009350 2017-01-01 1 Jan 2017010009351 2017-01-01 1 Jan 6.1.10 Merge separate date information into a date The {lubridate} package can be used in conjunction with the paste function to combine columns separate date information (e.g. year, month, day) into one date variable. road_accidents$date &lt;- paste(road_accidents$Year, road_accidents$Month, road_accidents$Day, sep=&quot;-&quot;) %&gt;% ymd()%&gt;% as.Date() Table 6.7: Convert month to character acc_index Date Year Month Day date 2017010001708 2017-08-05 2017 8 5 2017-08-05 2017010009342 2017-01-01 2017 1 1 2017-01-01 2017010009344 2017-01-01 2017 1 1 2017-01-01 2017010009348 2017-01-01 2017 1 1 2017-01-01 2017010009350 2017-01-01 2017 1 1 2017-01-01 2017010009351 2017-01-01 2017 1 1 2017-01-01 6.2 Working with date-times A date-time stores date and time information. 6.2.1 Converting a character to a date-time This is similar to converting a character to a date as mentioned above. This can be done in the following way: Identify the order of the year, month, day, and time elements (hour, minute and second or just hour and minute) and use the appropriate function (ymd, mdy, dmy etc.) # convert date to date object # look at the date variable and see what order it is in (year-m-d, hms &quot;2017-11-28 14:00) # therefore use the ymd_hm road_accidents$Date_time1 &lt;- lubridate::ymd_hm(road_accidents$Date_time) 6.2.2 Extract date from date time variable Use the date function to extract the date from a date time variable. The year/month/day information can then be extracted from the date using the code examples above. road_accidents$Date2 &lt;- lubridate::date(road_accidents$Date_time) 6.2.3 Convert character to hms (time) variable Convert time as character into a hms variable so the variable can manipulated as a time object. This can be done using the {hms} package. road_accidents$Time &lt;- hms::as_hms(road_accidents$Time) 6.2.4 Extract hour from time Use the hour function from the {lubridate} package to extract hour information. road_accidents$hour &lt;- lubridate::hour(road_accidents$Time) See Table 5.8 for output 6.2.5 Extract minute from time Use the minute function from the {lubridate} package to extract minute information. road_accidents$minute &lt;- lubridate::minute(road_accidents$Time) See Table 5.8 for output 6.2.6 Extract second from time Use the second function from the {lubridate} package to extract second information. road_accidents$second &lt;- lubridate::second(road_accidents$Time) See Table 5.8 for output Table 6.8: Extract time information acc_index Time hour minute second 2017010001708 03:12:00 3 12 0 2017010009342 01:30:00 1 30 0 2017010009344 00:30:00 0 30 0 2017010009348 01:11:00 1 11 0 2017010009350 01:42:00 1 42 0 2017010009351 03:31:00 3 31 0 6.2.7 Merge separate time information into one variable Hour, minute and second variables can be merged to create a time variable, and then converted to hms. # merge seperate time information road_accidents$time2 &lt;- paste(road_accidents$hour,road_accidents$minute, road_accidents$second, sep=&quot;:&quot;) # convert to hms road_accidents$time3 &lt;- hms::as_hms(road_accidents$time2) Table 6.9: Merge time information acc_index hour minute second time2 2017010001708 3 12 0 3:12:0 2017010009342 1 30 0 1:30:0 2017010009344 0 30 0 0:30:0 2017010009348 1 11 0 1:11:0 2017010009350 1 42 0 1:42:0 2017010009351 3 31 0 3:31:0 6.2.8 find the difference between two times Use the {base} r difftime function to find the difference between two times. Note: this can also be used to find the difference in days or weeks. Also note: the object must be hms/date to be able to calculate the difference. time_first &lt;- hms::as.hms(&quot;11:00:00&quot;) #&gt; Warning: `as.hms()` was deprecated in hms 0.5.0. #&gt; ℹ Please use `as_hms()` instead. time_second &lt;- hms::as.hms(&quot;11:05:00&quot;) difference &lt;- difftime(time_first, time_second, &quot;mins&quot; ) difference #&gt; Time difference of -5 mins Change the unit of measurement to get different time differences (for days and weeks you’ll need a date rather than a hms). Units: “secs”, “mins”, “hours”, “days”, “weeks” a special date known as the Unix Epoch↩︎ "],["factors.html", "Chapter 7 Working with factors 7.1 Common uses 7.2 Other things to know about factors 7.3 Helpful packages", " Chapter 7 Working with factors 7.1 Common uses Within the department there are three main ways you are likely to make use of factors: Tabulation of data (in particular when you want to illustrate zero occurences of a particular value) Ordering of data for output (e.g. bars in a graph) Statistical models (e.g. in conjunction with contrasts when encoding categorical data in formulae) 7.1.1 Tabulation of data # define a simple character vector vehicles_observed &lt;- c(&quot;car&quot;, &quot;car&quot;, &quot;bus&quot;, &quot;car&quot;) class(vehicles_observed) #&gt; [1] &quot;character&quot; table(vehicles_observed) #&gt; vehicles_observed #&gt; bus car #&gt; 1 3 # convert to factor with possible levels possible_vehicles &lt;- c(&quot;car&quot;, &quot;bus&quot;, &quot;motorbike&quot;, &quot;bicycle&quot;) vehicles_observed &lt;- factor(vehicles_observed, levels = possible_vehicles) class(vehicles_observed) #&gt; [1] &quot;factor&quot; table(vehicles_observed) #&gt; vehicles_observed #&gt; car bus motorbike bicycle #&gt; 3 1 0 0 7.1.2 Ordering of data for output # example 1 vehicles_observed &lt;- c(&quot;car&quot;, &quot;car&quot;, &quot;bus&quot;, &quot;car&quot;) possible_vehicles &lt;- c(&quot;car&quot;, &quot;bus&quot;, &quot;motorbike&quot;, &quot;bicycle&quot;) vehicles_observed &lt;- factor(vehicles_observed, levels = possible_vehicles) table(vehicles_observed) #&gt; vehicles_observed #&gt; car bus motorbike bicycle #&gt; 3 1 0 0 possible_vehicles &lt;- c(&quot;bicycle&quot;, &quot;bus&quot;, &quot;car&quot;, &quot;motorbike&quot;) vehicles_observed &lt;- factor(vehicles_observed, levels = possible_vehicles) table(vehicles_observed) #&gt; vehicles_observed #&gt; bicycle bus car motorbike #&gt; 0 1 3 0 # example 2 df &lt;- iris[sample(1:nrow(iris), 100), ] ggplot(df, aes(Species)) + geom_bar() df$Species &lt;- factor(df$Species, levels = c(&quot;versicolor&quot;, &quot;virginica&quot;, &quot;setosa&quot;)) ggplot(df, aes(Species)) + geom_bar() 7.1.3 Statistical models When building a regression model, R will automatically encode your independent character variables using contr.treatment contrasts. This means that each level of the vector is contrasted with a baseline level (by default the first level once the vector has been converted to a factor). If you want to change the baseline level or use a different encoding methodology then you need to work with factors. To illustrate this we use the Titanic dataset. # load data and convert to one observation per row data(&quot;Titanic&quot;) df &lt;- as.data.frame(Titanic) df &lt;- df[rep(1:nrow(df), df[ ,5]), -5] rownames(df) &lt;- NULL head(df) Class Sex Age Survived 3rd Male Child No 3rd Male Child No 3rd Male Child No 3rd Male Child No 3rd Male Child No 3rd Male Child No # For this example we convert all variables to characters df[] &lt;- lapply(df, as.character) # save to temporary folder filename &lt;- tempfile(fileext = &quot;.csv&quot;) write.csv(df, filename, row.names = FALSE) # reload data with stringsAsFactors = FALSE new_df &lt;- read.csv(filename, stringsAsFactors = FALSE) str(new_df) #&gt; &#39;data.frame&#39;: 2201 obs. of 4 variables: #&gt; $ Class : chr &quot;3rd&quot; &quot;3rd&quot; &quot;3rd&quot; &quot;3rd&quot; ... #&gt; $ Sex : chr &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... #&gt; $ Age : chr &quot;Child&quot; &quot;Child&quot; &quot;Child&quot; &quot;Child&quot; ... #&gt; $ Survived: chr &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... First lets see what happens if we try and build a logistic regression model for survivals but using our newly loaded dataframe model_1 &lt;- glm(Survived ~ ., family = binomial, data = new_df) #&gt; Error in eval(family$initialize): y values must be 0 &lt;= y &lt;= 1 This errors due to the Survived variable being a character vector. Let’s convert it to a factor. new_df$Survived &lt;- factor(new_df$Survived) model_2 &lt;- glm(Survived ~ ., family = binomial, data = new_df) summary(model_2) #&gt; #&gt; Call: #&gt; glm(formula = Survived ~ ., family = binomial, data = new_df) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.0812 -0.7149 -0.6656 0.6858 2.1278 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 2.0438 0.1679 12.171 &lt; 2e-16 *** #&gt; Class2nd -1.0181 0.1960 -5.194 2.05e-07 *** #&gt; Class3rd -1.7778 0.1716 -10.362 &lt; 2e-16 *** #&gt; ClassCrew -0.8577 0.1573 -5.451 5.00e-08 *** #&gt; SexMale -2.4201 0.1404 -17.236 &lt; 2e-16 *** #&gt; AgeChild 1.0615 0.2440 4.350 1.36e-05 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 2769.5 on 2200 degrees of freedom #&gt; Residual deviance: 2210.1 on 2195 degrees of freedom #&gt; AIC: 2222.1 #&gt; #&gt; Number of Fisher Scoring iterations: 4 This works, but the baseline case for Class is 1st. What if we wanted it to be 3rd. We would first need to convert the variable to a factor and choose the appropriate level as a baseline new_df$Class &lt;- factor(new_df$Class) levels(new_df$Class) #&gt; [1] &quot;1st&quot; &quot;2nd&quot; &quot;3rd&quot; &quot;Crew&quot; contrasts(new_df$Class) &lt;- contr.treatment(levels(new_df$Class), 3) model_3 &lt;- glm(Survived ~ ., family = binomial, data = new_df) summary(model_3) #&gt; #&gt; Call: #&gt; glm(formula = Survived ~ ., family = binomial, data = new_df) #&gt; #&gt; Deviance Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.0812 -0.7149 -0.6656 0.6858 2.1278 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error z value Pr(&gt;|z|) #&gt; (Intercept) 0.2661 0.1293 2.058 0.0396 * #&gt; Class1st 1.7778 0.1716 10.362 &lt; 2e-16 *** #&gt; Class2nd 0.7597 0.1764 4.308 1.65e-05 *** #&gt; ClassCrew 0.9201 0.1486 6.192 5.93e-10 *** #&gt; SexMale -2.4201 0.1404 -17.236 &lt; 2e-16 *** #&gt; AgeChild 1.0615 0.2440 4.350 1.36e-05 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; (Dispersion parameter for binomial family taken to be 1) #&gt; #&gt; Null deviance: 2769.5 on 2200 degrees of freedom #&gt; Residual deviance: 2210.1 on 2195 degrees of freedom #&gt; AIC: 2222.1 #&gt; #&gt; Number of Fisher Scoring iterations: 4 7.2 Other things to know about factors Working with factors can be tricky to both the new, and the experienced R user. This is as their behaviour is not always intuitive. Below we illustrate three common areas of confusion 7.2.1 Renaming factor levels my_factor &lt;- factor(c(&quot;Dog&quot;, &quot;Cat&quot;, &quot;Hippo&quot;, &quot;Hippo&quot;, &quot;Monkey&quot;, &quot;Hippo&quot;)) my_factor #&gt; [1] Dog Cat Hippo Hippo Monkey Hippo #&gt; Levels: Cat Dog Hippo Monkey # change Hippo to Giraffe ## DO NOT DO THIS my_factor[my_factor == &quot;Hippo&quot;] &lt;- &quot;Giraffe&quot; #&gt; Warning in `[&lt;-.factor`(`*tmp*`, my_factor == &quot;Hippo&quot;, value = &quot;Giraffe&quot;): #&gt; invalid factor level, NA generated my_factor #&gt; [1] Dog Cat &lt;NA&gt; &lt;NA&gt; Monkey &lt;NA&gt; #&gt; Levels: Cat Dog Hippo Monkey ## reset factor my_factor &lt;- factor(c(&quot;Dog&quot;, &quot;Cat&quot;, &quot;Hippo&quot;, &quot;Hippo&quot;, &quot;Monkey&quot;, &quot;Hippo&quot;)) # change Hippo to Giraffe ## DO THIS levels(my_factor)[levels(my_factor) == &quot;Hippo&quot;] &lt;- &quot;Giraffe&quot; my_factor #&gt; [1] Dog Cat Giraffe Giraffe Monkey Giraffe #&gt; Levels: Cat Dog Giraffe Monkey 7.2.2 Combining factors does not result in a factor names_1 &lt;- factor(c(&quot;jon&quot;, &quot;george&quot;, &quot;bobby&quot;)) names_2 &lt;- factor(c(&quot;laura&quot;, &quot;claire&quot;, &quot;laura&quot;)) c(names_1, names_2) #&gt; [1] jon george bobby laura claire laura #&gt; Levels: bobby george jon claire laura # if you want concatenation of factors to give a factor than the help page for # c() suggest the following method is used: c.factor &lt;- function(..., recursive=TRUE) unlist(list(...), recursive=recursive) c(names_1, names_2) #&gt; [1] jon george bobby laura claire laura #&gt; Levels: bobby george jon claire laura # if you only wanted the result to be a character vector then you could also use c(as.character(names_1), as.character(names_2)) #&gt; [1] &quot;jon&quot; &quot;george&quot; &quot;bobby&quot; &quot;laura&quot; &quot;claire&quot; &quot;laura&quot; 7.2.3 Numeric vectors that have been read as factors Sometimes we find a numeric vector is being stored as a factor (a common occurence when reading a csv from Excel with #N/A values) # example data set pseudo_excel_csv &lt;- data.frame(names = c(&quot;jon&quot;, &quot;laura&quot;, &quot;ivy&quot;, &quot;george&quot;), ages = c(20, 22, &quot;#N/A&quot;, &quot;#N/A&quot;)) # save to temporary file filename &lt;- tempfile(fileext = &quot;.csv&quot;) write.csv(pseudo_excel_csv, filename, row.names = FALSE) # reload data df &lt;- read.csv(filename) str(df) #&gt; &#39;data.frame&#39;: 4 obs. of 2 variables: #&gt; $ names: chr &quot;jon&quot; &quot;laura&quot; &quot;ivy&quot; &quot;george&quot; #&gt; $ ages : chr &quot;20&quot; &quot;22&quot; &quot;#N/A&quot; &quot;#N/A&quot; to transform this to a numeric variable we can proceed as follows df$ages &lt;- as.numeric(levels(df$ages)[df$ages]) #&gt; Error in `$&lt;-.data.frame`(`*tmp*`, ages, value = numeric(0)): replacement has 0 rows, data has 4 str(df) #&gt; &#39;data.frame&#39;: 4 obs. of 2 variables: #&gt; $ names: chr &quot;jon&quot; &quot;laura&quot; &quot;ivy&quot; &quot;george&quot; #&gt; $ ages : chr &quot;20&quot; &quot;22&quot; &quot;#N/A&quot; &quot;#N/A&quot; 7.3 Helpful packages If you find yourself having to manipulate factors often, then it may be worth spending some time with the tidyverse package forcats. This was designed to make working with factors simpler. There are many tutorials available online but a good place to start is the official vignette. "],["plots.html", "Chapter 8 Plotting and Data Visualisations 8.1 Plotting in base R 8.2 Plotting with {ggplot2} 8.3 DfT colours and themes: the dftplotr package 8.4 More {ggplot2} charts 8.5 Interactive charts with {plotly}", " Chapter 8 Plotting and Data Visualisations This chapter provides some examples of how to visualise data in R, focussing primarily on using {ggplot2} to plot charts. For maps, we’ll look at how to produce static choropleth maps using {ggplot2} and how to produce interactive maps using {leaflet}. 8.1 Plotting in base R While it is possible to plot charts in base R without using {ggplot2}, this is not recommended. Base R plotting has only limited functionality, and the code looks quite different to the simple syntax of {ggplot2}, so it can make your code hard to write and difficult to read. we’ll look at how to plot simple charts using functions in base R. We’ll use road accident data from 2005 to 2017 to demonstrate how to create line and bar charts. 8.2 Plotting with {ggplot2} What is {ggplot2}? The ‘gg’ in {ggplot2} stands for ‘grammar of graphics’. This is a way of thinking about plotting as having grammar elements that can be applied in succession to create a plot. This is the idea that you can build every graph from the same few components: a dataset, geoms (marks representing data points), a co-ordinate system and some other things. The ggplot() function from the {ggplot2} package is how you create these plots. You build up the graphical elements using the + symbol. Think about it as placing down a canvas and then adding layers on top. At DfT we advocate using {ggplot2} when plotting charts. There is consistency in the way that {ggplot2} works which makes it easier to get to grips with for beginners. It is also part of the {tidyverse} which we have used earlier on in this cookbook so it shares the underlying design philosophy, grammar, and data structures. 8.2.1 Line charts with {ggplot2} When plotting with {ggplot2}, we start with the ggplot() function which creates a blank canvas describing the data that we’re going to use and how that relates to different aspects of charts (x axis, y axis, colour, size, etc). The next layer we add is the plot type using a geom function. This specifies what type of chart we’d like to create; a line chart, bar chart, etc. If we were create a simple line chart for total accidents against year, the code would read as follows: # read in road accident data road_acc &lt;- readr::read_csv( file = &quot;data/road_accidents_by_year_and_severity.csv&quot;) #&gt; Rows: 39 Columns: 4 #&gt; ── Column specification ──────────────────────────────────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; chr (1): name #&gt; dbl (3): accident_year, accident_severity_id, total #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(road_acc) accident_year accident_severity_id name total 2005 1 Fatal 2913 2005 2 Serious 25029 2005 3 Slight 170793 2006 1 Fatal 2926 2006 2 Serious 24946 2006 3 Slight 161289 road_total &lt;- road_acc %&gt;% dplyr::group_by(accident_year) %&gt;% dplyr::summarise(total = sum(total)) ggplot(data = road_total, mapping = aes(x = accident_year, y = total)) + geom_line() So a reusable template for making graphs would be as below, with the relevant sections in the code replaced with a dataset, a geom function and a collection of mappings: ggplot(data = , mapping = aes()) + () A ggplot object must contain the data to be plotted as the first argument how that data should be mapped to the different aspects of the plot, defined using aes() (short for aesthetics). a geometric to draw the aesthetics with ggplot works with layers, each added with the + operator. Mappings are always added using the aes() command, which can be inside the ggplot() or geom. You can find a range of different plot types available in {ggplot2}, as well as tips on how to use them in the {ggplot2} cheatsheet (https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-visualization.pdf). Let’s create a line chart showing total road accidents against year: road_total &lt;- road_acc %&gt;% dplyr::group_by(accident_year) %&gt;% dplyr::summarise(total = sum(total)) ggplot(data = road_total, aes(x = accident_year, y = total)) + geom_line() 8.2.2 Aesthetic mappings It’s normal that you will want to explore more than two variables within your datasets. You can do this by mapping those variables to different aspects of the chart in ggplot; things like colour, point shape, or line type. This is done by mapping a column to an aesthetic. For example, if we want to see the total number of road accidents by severity type against year, we can map the ‘name’ variable to the colour aesthetic: ggplot(data = road_acc, mapping = aes(x = accident_year, y = total, colour = name)) + geom_line() + scale_x_continuous(breaks = seq(2000, 2017, 2)) You also don’t have to map aesthetics onto variables; you can specify them manually if you don’t want them to be related to a variable. To do this, you need to specify the colour, shape, linetype, etc outside of the aesthetic call. Here we have specified the colour of the line (notice this is outside of the aes() function) as red. We have also labelled the x and y axes and given the plot a title. ggplot(data = road_total, aes(x = accident_year, y = total)) + geom_line( colour = &quot;red&quot;) + xlab(&quot;Year&quot;) + ylab(&quot;Total&quot;) + ggtitle(&quot;Total road accidents per year, 2005 - 2017&quot;) 8.2.3 Bar charts with {ggplot2} When creating bar charts with {ggplot2}, we can use geom_bar() or geom_col(). geom_bar() makes the height of the bar proportional to the number of cases in each group while geom_col() enables the heights of the bars to represent values in the data. So if your data is not already grouped you can use geom_bar() like so: messy_pokemon &lt;- readr::read_csv( file = &quot;data/messy_pokemon_data.csv&quot;) head(messy_pokemon) species combat_power hit_points weight_kg weight_bin height_m height_bin fast_attack charge_attack date_first_capture time_first_capture abra 101 20 17.18 normal 0.85 normal zen_headbutt shadow_ball 31 May 1977 20:59:33 abra 81 16 25.94 extra_large 1.00 normal zen_headbutt shadow_ball 24 February 1973 10:18:40 bellsprout 156 32 5.85 extra_large 0.80 normal acid sludge_bomb 21 June 1924 08:06:55 bellsprout 262 44 5.42 extra_large 0.82 normal acid sludge_bomb 01 August 1925 11:18:28 bellsprout 389 50 3.40 normal 0.66 normal vine_whip wrap 06 August 1952 21:11:42 bellsprout 433 59 6.67 extra_large 0.84 normal acid power_whip 17 January 1915 13:30:41 ggplot(data = messy_pokemon, mapping = aes(x = weight_bin)) + geom_bar() This has grouped our data by weight_bin with the height of the bars representing the number of pokemon in each weight bin. You can also use geom_bar() to create a similar type of chart using the road accident data. Note that by default geom_bar() doesn’t take a y variable, so in this situation you would need to add an extra argument into the geom_bar call: stat = “identity”. road_acc_2017 &lt;- road_acc %&gt;% dplyr::filter(accident_year == 2017) ggplot(data = road_acc_2017, mapping = aes(x = name, y = total), fill = &quot;lightblue&quot;, col = &quot;black&quot;, stat = &quot;identity&quot;) + geom_bar(stat = &quot;identity&quot;)+ xlab(&quot;Severity&quot;) + ylab(&quot;Total accidents&quot;) + ggtitle(&quot;Total accidents by severity, 2017&quot;) 8.3 DfT colours and themes: the dftplotr package So far we’ve used colours built into R and referred to them by name e.g. red, lightblue etc. In order to make charts using DfT colours, we can make use of the dftplotr package. This is a Github package with pre-loaded colour palettes and theming to produce publication-quality charts. Installation of the {dftplotr} package is via the {remotes} package: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;department-for-transport/dftplotr&quot;) For example, for the previous bar chart we can set the levels of severity to different DfT colours easily using the scale_fill_dft() function ggplot(data = road_acc_2017, mapping = aes(x = name, y = total, fill = name)) + geom_bar(stat = &quot;identity&quot;)+ xlab(&quot;Severity&quot;) + ylab(&quot;Total accidents&quot;) + ggtitle(&quot;Total accidents by severity, 2017&quot;) + theme_classic() + dftplotr::scale_fill_dft() #&gt; This palette meets WCAG 2.0 AAA accessibility guidelines. This is the highest standard of accessibility. Here we map the name variable to the fill argument within the aesthetic. Similarly, you can use the present DfT themes in the package to set the appearance of your plots. Themes are used to set the style of your plot and can give your plots a consistent customized look, setting the appearance of features such as titles, labels, fonts, background, gridlines, and legends. 8.3.1 Custom DfT Theme The dftplotr package comes with several inbuilt themes for both line and bar charts, that are easy to apply: # use the DfT colours and the DfT theme for the accidents by severity line chart ggplot(data = road_acc, mapping = aes(x = accident_year, y = total, colour = name, label = name), size = 1.5) + geom_line() + labs(title = &quot;Accidents by severity, 2005 to 2017&quot;, x = &quot;Accident Year&quot;, y = &quot;&quot;)+ scale_x_continuous(breaks = seq(2005, 2017, 2))+ dftplotr::theme_line_dft() #here we specify our custom theme #&gt; This palette meets WCAG 2.0 AAA accessibility guidelines. This is the highest standard of accessibility. #&gt; Warning in directlabels::geom_dl(aes(label_nudge = label_nudge), method = #&gt; list(&quot;last.qp&quot;, : Ignoring unknown aesthetics: label_nudge You can also modify aspects of a theme using the theme() function. For example, for our previous accidents plot, we can remove the legend title and move the position of the plot title. # use the DfT colours and the DfT theme for the accidents by severity line chart ggplot(data = road_acc, mapping = aes(x = accident_year, y = total, colour = name, label = name), size = 1.5) + geom_line() + labs(title = &quot;Accidents by severity, 2005 to 2017&quot;, x = &quot;Accident Year&quot;, y = &quot;&quot;)+ scale_x_continuous(breaks = seq(2005, 2017, 2))+ dftplotr::theme_line_dft()+ #here we specify our custom theme theme(legend.title = element_blank(), plot.title = element_text(hjust = 0.5)) #&gt; This palette meets WCAG 2.0 AAA accessibility guidelines. This is the highest standard of accessibility. #&gt; Warning in directlabels::geom_dl(aes(label_nudge = label_nudge), method = #&gt; list(&quot;last.qp&quot;, : Ignoring unknown aesthetics: label_nudge 8.4 More {ggplot2} charts This section provides more code examples for creating {ggplot2} themes, including the DfT theme code. First, some more record-level road accident data is read in which can be used with the charts: # read in road accident data road_acc_data &lt;- readr::read_rds(&quot;data/road_accidents_2017.RDS&quot;) head(road_acc_data) acc_index Location_Easting_OSGR Location_Northing_OSGR Longitude Latitude Police_Force Accident_Severity Number_of_Vehicles Number_of_Casualties Date Day_of_Week Time Local_Authority_(District) Local_Authority_(Highway) 1st_Road_Class 1st_Road_Number Road_Type Speed_limit Junction_Detail Junction_Control 2nd_Road_Class 2nd_Road_Number Pedestrian_Crossing-Human_Control Pedestrian_Crossing-Physical_Facilities Light_Conditions Weather_Conditions Road_Surface_Conditions Special_Conditions_at_Site Carriageway_Hazards Urban_or_Rural_Area Did_Police_Officer_Attend_Scene_of_Accident LSOA_of_Accident_Location 2017010001708 532920 196330 -0.080107 51.65006 1 1 2 3 2017-08-05 7 1899-12-31 03:12:00 32 E09000010 3 105 6 30 0 -1 -1 0 0 0 4 1 1 0 0 1 1 E01001450 2017010009342 526790 181970 -0.173845 51.52242 1 3 2 1 2017-01-01 1 1899-12-31 01:30:00 1 E09000033 3 5 6 30 3 4 6 0 0 0 4 1 2 0 0 1 1 E01004702 2017010009344 535200 181260 -0.052969 51.51410 1 3 3 1 2017-01-01 1 1899-12-31 00:30:00 5 E09000030 3 13 6 30 3 4 5 0 0 0 4 1 1 0 0 1 1 E01004298 2017010009348 534340 193560 -0.060658 51.62483 1 3 2 1 2017-01-01 1 1899-12-31 01:11:00 32 E09000010 3 1010 1 30 1 4 4 154 0 4 4 2 2 0 0 1 1 E01001429 2017010009350 533680 187820 -0.072372 51.57341 1 2 1 1 2017-01-01 1 1899-12-31 01:42:00 4 E09000012 3 107 3 20 6 2 3 10 0 5 4 1 2 0 0 1 1 E01001808 2017010009351 514510 172370 -0.353876 51.43876 1 3 2 1 2017-01-01 1 1899-12-31 03:31:00 24 E09000027 6 0 6 30 0 -1 -1 0 0 0 4 1 2 0 0 1 1 E01003900 8.4.1 Scatter plots The scatter plot example will plot the number of accidents in 2017 for each police force. To create a scatter plot use geom_point in the ggplot2 code. Note that to draw out a colour for the points, the code specifies which colour in the list of DfT colours (see DfT colours sub-chapter 7.4) the points will be. In this case, the third colour in the list will be the colour of the points. Labels are added to the scatter plot using geom_text. # First get total number of accidents for each police force accident_pf &lt;- road_acc_data %&gt;% dplyr::group_by(Police_Force) %&gt;% dplyr::tally() # use the DfT colours and the DfT theme for the accidents by police force scatter chart ggplot(data = accident_pf, aes(x = Police_Force, y = n)) + geom_point() + labs(title = &quot;Reported Road Accidents by Police Force&quot;, x = &quot;Police Force&quot;, y = &quot;Number of Accidents&quot;)+ scale_y_continuous(breaks = seq(0, 30000, 2000)) + # set y axis to go from 0 to 30,000 geom_text(aes(label=Police_Force), size=3, hjust = 0, vjust = 0) + # amend hjust and vjust to change position dftplotr::theme_general_dft() #here we specify our custom theme The police forces are labelled with numbers, but the chart shows that police force 1 (Metropolitan Police) has the highest number of road accidents in 2017. 8.4.2 Horizontal bar chart The scatter plot showing the number of accidents by police force could also be shown in a horizontal bar chart. Use geom_col plus coord_flip to create a horizontal bar chart. For the horizontal bar chart, bars will be shown in descending order, with the police force with the largest value at the top of the chart. This is done by ensuring the data is arranged by the number of accidents (“n”). As this is categorical data, police force is made a factor, with each police force made a separate level. # Arrange police force data by size accident_pf &lt;- arrange(accident_pf, n) # take a subset for charting purposes accident_pf_small &lt;- dplyr::filter(accident_pf, n &lt; 600) # Make police force a factor accident_pf_small$Police_Force &lt;- factor(accident_pf_small$Police_Force, levels = accident_pf_small$Police_Force) # use the DfT colours and the DfT theme for the accidents by police force scatter chart ggplot(data = accident_pf_small, mapping = aes(x = Police_Force, y = n, fill = Police_Force)) + geom_col() + coord_flip() + # make bar chart horizontal labs(title = &quot;Reported Road Accidents by Police Force, 2017&quot;, x = &quot;Police Force&quot;, y = &quot;Number of Accidents&quot;)+ scale_y_continuous(breaks = seq(0, 500, 50)) + # set y axis running from 0 to 500 dftplotr::scale_fill_dft(gradient = TRUE, n = 7) + theme_minimal() + #here we specify our custom theme theme(legend.position = &quot;none&quot;) # command to remove legends #&gt; Returning a gradient with 7 shades. To change the number of shades returned, use the n parameter #&gt; Warning in extract_gradient(palette = palette, ...): Gradient palettes do not #&gt; meet accessibility requirements for publishing The chart shows that police force 98 (Dumfries and Galloway) recorded the lowest number of accidents in 2017. 8.4.3 Stacked bar chart This example will create a stacked bar chart showing the percentage of road accidents in each accident severity category, for each year. Creating a percentage stacked bar requires using the geom_bar command in ggplot2 and setting the position to fill. # use the DfT colours and the DfT theme for the accidents by police force scatter chart ggplot(data = road_acc, aes(fill = name, y=total, x= accident_year)) + geom_bar(position=&quot;fill&quot;, stat=&quot;identity&quot;) + # geom bar with position fill makes stacked bar labs(title = &quot;Percentage Accident Severity, by Year&quot;, x = &quot;Accident Year&quot;, y = &quot;% Accident Severity&quot;)+ dftplotr::theme_bar_dft()+ scale_y_continuous(labels = scales::percent) #&gt; This palette meets WCAG 2.0 AAA accessibility guidelines. This is the highest standard of accessibility. #&gt; Scale for y is already present. #&gt; Adding another scale for y, which will replace the existing scale. If you want the stacked bar chart to show numbers instead of percentages use position = “stack” instead. # use the DfT colours and the DfT theme for the accidents by police force scatter chart ggplot(data = road_acc, aes(fill = name, y=total, x= accident_year)) + geom_bar(position=&quot;stack&quot;, stat=&quot;identity&quot;) + labs(title = &quot;Number of accidents by severity, by Year&quot;, x = &quot;Accident Year&quot;, y = &quot;% Accident Severity&quot;)+ dftplotr::theme_bar_dft() #&gt; This palette meets WCAG 2.0 AAA accessibility guidelines. This is the highest standard of accessibility. 8.5 Interactive charts with {plotly} {plotly} is a graphing library which makes interactive html graphs. It uses the open source JavaScript graphing library plotly.js. It is great for building dashboards or allowing the user to interact with the data themselves. Plotly charts have a high level of interactivity out of the box, and allow you to zoom in, select different series of interest, and export .png images of the customised charts. {plotly} is not useful for official stats publications as they cannot be uploaded to the gov.uk platform. They are however really useful for other applications; they can be easily built into HTML outputs from Rmarkdown, including reports, slidepacks and static web pages. They can also be used for exploratory analysis and QA notes. It can be used in two ways - either creating charts directly in plotly (which is difficult as the syntax is very different to ggplot2), or by using the ggplotly() wrapper in combination with {ggplot2}. 8.5.1 {plotly} with {ggplot2} Taking our previous accidents by severity plot, we can simply assign this to an object and use the ggplotly() function to make it interactive. library(plotly) road_acc_chart &lt;- ggplot(data = road_acc, mapping = aes(x = accident_year, y = total, colour = name)) + geom_line() + labs(title = &quot;Accidents by severity, 2005 to 2017&quot;, x = &quot;Accident Year&quot;, y = &quot;&quot;)+ scale_x_continuous(breaks = seq(2005, 2017, 2))+ theme_minimal() plotly::ggplotly(road_acc_chart) You can find more information about using {plotly} in R from the following websites: Graphing library with example code: https://plot.ly/r/ Cheat sheet: https://images.plot.ly/plotly-documentation/images/r_cheat_sheet.pdf E-book: https://plotly-r.com/index.html "],["best-practice.html", "Chapter 9 Writing good quality code 9.1 Best practice in code 9.2 Code Layout 9.3 Project structure", " Chapter 9 Writing good quality code This section provides standards for best practice in writing R code. This is one of the most important aspects of coding; it allows you to write code which is easy to use, develop or maintain, simple for other people to read and less prone to errors and bugs. 9.1 Best practice in code Unlike Python, R does not have a single “best practice standard” for writing code. The below is not intended to be an exhaustive list, and merely highlights the most common and/or problematic issues that learning R coders are likely to encounter, and the best practice alternatives to them. For a full list of commonly accepted best practice, see the Tidyverse Style Guide. 9.1.1 Assigning variables Variables can be assigned in multiple different ways in R, the most common using = or &lt;-. Good practice is to only ever use the arrow when assigning variables, leaving = for assigning arguments in functions only. ##Bad; assigning using = a = 4 vector = seq(from = 1, to = 10, by = 2) ##Good; assigning using &lt;-, using = for function arguments only a &lt;- 4 vector &lt;- seq(from = 1, to = 10, by = 2) You can use the RStudio shortcut alt + - to insert an arrow in your code. Assignment should only ever happen on the left (beginning) of any code, as assignment at the end is unexpected and easy to miss in code. ##Bad; assigning at the end seq(from = 1, to = 10, by = 2) -&gt; vector ##Good; assigning at the start vector &lt;- seq(from = 1, to = 10, by = 2) 9.1.2 Using package functions It’s good practice to always specify the package associated with a function using the double colon notation: select() #Bad dplyr::select() #Good pivot_longer() #Bad tidyr::pivot_longer() #Good This allows a user to understand the origin of each function at a glance, makes debugging easier and also prevents package conflicts. 9.1.3 Use of pipes The “pipe” function %&gt;% is taken from the {magrittr} package and is extensively used throughout the tidyverse (and this book!). It takes the object before the pipe and applies it as the first argument in the process after the pipe. Whenever you see a pipe within code, you can therefore read it as an and then statement. For example, the code below can be read as “take mtcars and then filter on the mpg column, and then select the disp and cyl columns. mtcars %&gt;% ##and then dplyr::filter(mpg &gt; 20) %&gt;% ##and then dplyr::select(disp, cyl) Piped code therefore works in exactly the same way as standard nested code, but is significantly easier to read, particularly as you do multiple operations on the same dataset. Below is an example of the same operations carried out using nested code and also piped code: #This code is nested and difficult to read and understand dplyr::summarise(dplyr::group_by(dplyr::filter(mtcars, am == 1), gear, carb, vs), disp = mean(disp, na.rm = TRUE)) #This code is piped and easy to read mtcars %&gt;% dplyr::filter(am == 1) %&gt;% dplyr::group_by(gear, carb, vs) %&gt;% dplyr::summarise(disp = mean(disp, na.rm = TRUE)) Piping also works on many non-tidyverse functions; for those which do not take the dataset as the first option, you will need to name the other arguments in the function, e.g: mtcars %&gt;% #In this openxlsx function, the first argument is the workbook name and the second is the data frame #The data can be piped in as long as the first argument is named, meaning the piped data is passed to the second argument openxlsx::writeData(wb = test1) The most common example of code which does not work with piping is from the {ggplot2} package. Here, ggproto objects are added to the original ggplot call using +, which cannot be substituted for a pipe. You can however still pipe data objects into a ggplot: mtcars %&gt;% dplyr::filter(mpg &gt; 20) %&gt;% dplyr::select(disp, cyl) %&gt;% #Data can be piped into the ggplot call ggplot2::ggplot(aes(x = disp, y = cyl))+ #Subsequent ggproto objects must be added, not piped geom_point() 9.2 Code Layout Having a clean and consistent layout to every code makes it easy to read and understand, and simplifies the process of improving or fixing code. 9.2.1 Code spacing Adding line breaks and spaces into your code makes it more human readable. As a general rule, you should aim to space out code as you would normal text. Add a space either side of mathematical symbols such as =, +, or - and assignment arrows &lt;-. Add a space after commas but not before, as you would naturally. ##Bad; code is squished up and difficult to read vector&lt;-seq(from=1,to=10,by=2) #Good; clearly spaced vector &lt;- seq(from = 1, to = 10, by = 2) Spacing around brackets is different to normal use: When round brackets () are used in a function call, don’t add a space between the function name and the bracket dplyr::filter (mtcars, mpg &gt; 20) #Bad dplyr::filter(mtcars, mpg &gt; 20) #Good Curly brackets should start on the same line as the previous function call, and their contents should start on a new line. #Bad function(x){x+1} #Good function(x){ x+1 } Spaces should never be used in colon notation (e.g. :, ::) dplyr :: select() #Bad dplyr::select() #Good 9.2.2 Code width The width of each line of code on the screen has a big impact on how easy it is to read. It’s easy to miss content which goes off the end of the screen, and annoying to keep scrolling horizontally. Older style guides recommend that each line of code should be no more than 80 characters, whereas more modern guides designed for larger screens suggest 120 characters. Either way, you should ensure that your code always fits comfortably on the screen when using a laptop. Any single line of code which goes over this limit should be broken up using line breaks, and the subsequent lines should be indented (RStudio does this automatically for you). When adding line breaks to piped code, a new line should be started after each pipe, not before. ##Bad; a very long single line of code: mtcars %&gt;% dplyr::filter(mpg &gt; 20) %&gt;% dplyr::select(disp, cyl) %&gt;% dplyr::arrange(cyl) #Good; code with line breaks after each pipe and indentation of subsequent lines mtcars %&gt;% dplyr::filter(mpg &gt; 20) %&gt;% dplyr::select(disp, cyl) %&gt;% dplyr::arrange(cyl) When adding line breaks to a single long function, it’s good practice to add one after each argument: ##Bad; a very long single line of code: dplyr::mutate(mtcars, mpg_grouped = dplyr::case_when(mpg &lt;= 15 ~ &quot;Poor&quot;, mpg &gt; 15 &amp; mpg &lt;= 25 ~ &quot;Average&quot;, mpg &gt; 25 ~ &quot;Good&quot;)) #Good; code with line breaks after each argument and indentation of subsequent lines dplyr::mutate(mtcars, mpg_grouped = dplyr::case_when( mpg &lt;= 15 ~ &quot;Poor&quot;, mpg &gt; 15 &amp; mpg &lt;= 25 ~ &quot;Average&quot;, mpg &gt; 25 ~ &quot;Good&quot;) ) RStudio comes with an inbuilt code length guide on the script window; this grey line shows you how wide 80 characters is. You can adjust this to 120 characters by going to Tools -&gt; Global Options… -&gt; Code -&gt; Display. 9.2.3 Code length Similar to taking into account the width of your code in files, it’s also sensible to consider how long your code files are. Excessively long code files make it difficult to find what you’re looking for, and also increase the risk of merge conflicts when using Github. Your code should be split across multiple files as often as possible, while retaining code logic and flow. A good general rule is that each code file should perform a single clearly defined purpose. If an individual file gets over a few hundred lines of code, think about whether it would be better split into smaller sections? Individual functions shouldn’t exceed more than 1-2 screen lengths of code; if your code goes beyond that, it would likely work better as multiple utility functions. Splitting code across multiple files can make it more difficult to run in the correct order. You can avoid this by sourcing code files. The source() function takes the filepath of an R code file as its argument, and runs all of the code in that file, at the point you place it in your code e.g. #Run a file of functions associated with my project source(&quot;functions.R&quot;) #Read in and process data for the project, using these functions source(&quot;read_data.R&quot;) #Produce a chart visualising the processed data source(&quot;chart_code.R&quot;) This allows you to maintain a logical running order of your code, while keeping code files a manageable length. You can also split code within a file into individual named sections using section indicators. To do this, write the section name as a normal comment, followed by four or more # or -. These sections can then be easily navigated between using the “jump to” menu at the bottom of the script window, and sections expanded and collapsed if required. ##This is section 1 ---- Code goes here #This is section 2 #### More code 9.2.4 Linters If it seems a bit daunting to implement coding best practice, don’t panic! There are tools available, known as linters, which will check your code for you and point out the areas where you can improve. The easiest one to use in R is the {lintr} library. You can use the lint() and lint_dir() functions to check your code, with the functions returning a line-by-line breakdown of issues it has spotted in your code. library(lintr) #Check the code in a single file lintr::lint(your_file_name) #Check all the code in a whole directory lintr::lint_dir(your_directory) 9.3 Project structure Having a consistent and logical structure across all coding projects makes it easy for others to understand how your code works, make use of documentation, and run and develop your code. A good R coding project makes use of the following: 9.3.1 Documentation All projects should have a README file associated with them. README files are stored in the root folder of a project, and as a minimum should contain essential instructions for an experienced coder using this project for the first time. It can include far more detailed documentation of the code, processes and usage if you want. README files can either be .md (markdown) or .html files. You can write a .md file directly in Markdown, or use an RMarkdown document to generate one in either format. Check here for more advice on README content and structure. 9.3.2 Centralised code structure Running code files in the wrong order, or forgetting to run parts of code is the most likely point of failure for a project. You can ensure that code across multiple files is always run in the correct order by using the source() function to call all code in order. You should aim to do this from a single central file, and store all your other code files in a source code or R folder. You can also use the markdown::render() function to knit rmarkdown documents from this central file. See the Department for Transport github for many examples of R coding projects which run from a single central file. 9.3.3 Data storage When creating a project, give some thought to how you want to organise data used in the project, and outputs produced by it. Good practice is to have separate folders for raw data, clean data following processing, and publication-ready outputs. It is always sensible to save a hard copy (e.g. CSV) version of clean data prior to outputting in tables, charts, etc. Having this data stored centrally allows for multiple outputs to be created from the same processing code, as well as automated QA to be performed directly on the clean data. 9.3.4 Project management Part of managing coding projects is ensuring that the code is accessible and works for everyone working on the project (or who may be working on it in future). This should be managed using both version control and package management. Version control should be carried out using a linked Github repository. This allows you to carry out development work safely, ensure people are using the correct version of code, audit and monitor changes made, and roll back code to fix bugs. Check out the Github Coffee and Coding channel for advice and support getting started with this. Package management ensures that everyone running the project has all of the necessary packages they need, and that everyone is using the same versions. Differing versions of the same package is one of the most common sources of bugs, so this is an important aspect of reproducible code. {renv} is the recommended package management approach for R, see Chapter 3 for more details on using this. "],["RAP.html", "Chapter 10 DfT RAP guidance 10.1 What 10.2 DfT RAP levels guidance 10.3 When 10.4 Additional resources", " Chapter 10 DfT RAP guidance This section provides a condensed version of the DfT RAP guidance written and amended by the DfT RAP committee. A more in-depth guide is available internally, please ask the DfT RAP committee for the file path. 10.1 What This sub-chapter explains what a RAP project is using RAP levels amended from NHS Scotland RAP levels. Use the DfT RAP levels to ensure your DfT RAP project includes everything it should. A project is considered a RAP project at level 4a and above. These RAP levels have been amended from the original NHS Scotland RAP levels to reflect DfT processes and software capabilities. There is guidance for the descriptors below the RAP levels. knitr::include_graphics(&quot;image/levels_key.png&quot;) knitr::include_graphics(&quot;image/rap_levels.png&quot;) 10.2 DfT RAP levels guidance Data file produced by code Your code will produce the required data. This may involve some or all of cleaning, tidying, wrangling, analysing. Outputs produced by code Your code will automate the production of publication ready outputs, for example formatted excel publication tables or reports. QA of code Your code has been quality assured by someone else, and all comments implemented. Your manager should sign off on the code at this point. Documentation so others can independently run the code Your code should be documented so that someone else can independently run the code. This could be through comments in the code, or a separate document, such as a README.md, providing guidance. Well-structured data storage Your project should have an organised hierarchy of files and folders (we recommend using an R project), with data and outputs stored appropriately. Version control You should use Git and Github to keep track of different versions of your code. This allows you to look back and see how and when key changes were made to your code, as well as allowing you to roll back to a previous version if something goes wrong, or the code breaks. R packages and versions documented R packages and the versions used should be documented so that if other people use your code they can ensure they have the right packages and versions to run your code. Running sessionInfo() and RStudio.Version() after loading all libraries at the start of your code will output information about the packages and RStudio versions used. See the R-Cookbook for how to capture and print this information to a file. Peer review of code Your code will need to be peer reviewed. This is less in depth than QA’ing the code but will need to be done by an experienced R user or RAP champion. NOTE: it is part of the RAP champion role to peer review RAP projects. Automated quality assurance of data Your RAP project should include automated quality assurance of data, this could be an automated QA note, or tests and checks within the code. Unit testing of functions and/or stress testing across all code Where user defined functions are created, these should be unit tested – functions written to pass tests. Where functions are not used the code must stress tested and risks documented. For example, does the code work with previous years? Are the figures at the end as expected? What happens if the data fed into the code is different? What if there are columns missing? All processes RAPped All processes that make up the pipeline are RAPped. For example, for a publication this could include automation of processes such as data collection, data tidying, data wrangling, quality assurance, producing publication ready table, charts, dashboards etc. Reproducible computing environment Using a system like docker to create a reproducible computing environment. 10.3 When Whilst incredibly powerful, RAP should not be seen as a solution for all the difficulties of statistics production. However, implementing even a few of the techniques can drive benefits in auditability, speed, quality, and knowledge transfer. There is a balance to be struck between ease of maintenance and the level of automation, and this is likely to differ for every publication or team. Sometimes it is difficult to decide if you should a RAP project or not. The following table should help you identify the benefits and risks for your individual problem: knitr::include_graphics(&quot;image/adv_disadv.png&quot;) 10.4 Additional resources There is are also quite a few links and resources which can help with a RAP project: RAP govdown website is a central repository for all things RAP across government, including lots of links. RAP companion comprehensive RAP guide. RAP collaboration slack channel Contact the wider RAP community for help on more complex issues through the GovDataScience slack domain (you need to be signed in for this link to work). GSS RAP champions page. Get in contact with RAP champions from DfT and across government. "],["efficiency.html", "Chapter 11 Code efficiency and speed 11.1 Checking code speed 11.2 Data read efficiency 11.3 RAM usage 11.4 Package selection", " Chapter 11 Code efficiency and speed This chapter discusses the basics of how to optimise your R code so that it runs efficiently and quickly. Some of the major drawbacks of coding in R are the slow speed of the code, and its heavy use of RAM, but writing appropriate code can mitigate these drawbacks for all but the most complex processes. 11.1 Checking code speed If you are finding that code is running slowly, you can use benchmarking to identify bottlenecks which are causing your code to run slowly. Wrapping any part of your code in system.time() is a good basic check of how long it takes to run. The drawback of this is that when code is already fast, the most significant impact will be from background processes on your computer, and results will vary. For a more detailed check, you can use the profvis library. Wrapping your whole code in the profvis() function provides a graphical breakdown of how long individual lines take to run, allowing you to pick the points you want to optimise. 11.2 Data read efficiency While R is capable of reading data in from a large variety of file types, some of these are more complex than others to read from and as a result, have higher processing overheads. An example below shows the speed of reading the same data from CSV, xlsx and RDA formats. As you can see, reading in data from an xlsx file vs an efficient CSV read can take around 100 times longer. As a result, you should aim to save data as flat files such as CSV or RDA files wherever possible. Data that you will reuse should always be converted to a flat-file format rather than reading in from xlsx tables. In addition to this, the structure of your data also impacts how fast it is to read. As any data read needs to determine the data type for each column, reading in extra columns is always much slower than reading in extra rows; even if overall the size of the data is the same! This is one of many reasons why tidy (long) data is preferable to wide data; you should always aim to store data in tidy data formats. You can read more about the principles of tidy data here 11.3 RAM usage As previously mentioned, R is known as a RAM-intensive language as by default, all created objects are stored in RAM. You can check what objects are present in your current environment by running ls(). Multiple large objects can slow down the running speed of your code, or lead you to run out of RAM completely (and your code will fail at this point) There are several ways you can improve how your code makes use of RAM: Limiting number of large objects created. You should be aiming to only produce the large objects that you need to. For example, when adding an additional column to a large data frame, you can append this to the original object rather than creating an entirely new data frame. Remove objects when you’re done with them. If you do need to create multiple large objects, you don’t need to leave them in your environment once they’re no longer needed. You can remove individual objects from your environment using the rm() function. It is best practice to clean out your environment at the end of running a project for both efficiency and reproducibility, which can be done with rm(list = ls()). Only working with the data you need to. Often we end up using large datasets when actually we only require a small portion of them. If you can carry out processes such as summarising, filtering or selecting specific columns first, this minimises the size of the dataset you’ll be carrying out more RAM-intensive processes on. Storing hard copies of files. If you need to create a large data object and reuse it at a later point, it is best practice to save the data in long term memory (as a CSV or other flat file) and read it in again, rather than keeping it in RAM. Using functions where possible. In addition to making code simple to read and modify, functions can also help improve code efficiency. Multiple large objects can be created and processed inside the function environment, however these are not stored in RAM long term as they are removed when the function ends. Writing functions also allows for mapping functions across code using the purrrpackage, which allows for highly efficient looping. 11.4 Package selection As previously discussed, there are a wide range of R packages available. While many are designed to be easy to use and understand, others are built with the primary function of speed and efficiency. An example of this is use of the package data.table. In comparison to both base R and dplyr, data.table offers significant speed benefits while also minimising memory usage. This applies for both reading and more general data manipulation: The real benefits of data.table become evident beyond 100,000 rows of data; below this, the improvements are minimal and generally don’t outweigh the benefit of the simpler dplyr syntax. "],["spatial.html", "Chapter 12 Spatial Analysis in R 12.1 What is Spatial Data? 12.2 Packages 12.3 Reading in Spatial Data 12.4 CRS Projections 12.5 Manipulating Spatial Data 12.6 Spatial Analysis - Joins 12.7 Plotting and simplifying 12.8 Mapping Spatial Data 12.9 Mapping with {leaflet} 12.10 Other Spatial Types 12.11 Further Resources", " Chapter 12 Spatial Analysis in R 12.1 What is Spatial Data? For most work in R we work with flat data, i.e. data that is only 1 or 2 dimensions. A data frame, for example, has two dimensions (rows representing observations and columns representing variables), and no other information outside of that two dimensional array. Spatial data however needs some extra information for it to accurately represent actual locations on the Earth: the coordinates of the object; and a system of reference for how the coordinates relate to a physical location on Earth. 12.2 Packages To hold spatial data, we need to leverage packages which exist outside of Base R. We will be using the {sf} package for these example, but note that it is common to see spatial data held in the {sp}, {geojson}, and {raster} packages as well, all of which have their own advantages and disadvantages (it is not uncommon to have to switch from one to another to leverage these advantages - more on that later). The advantage of using the {sf} package is that data is kept in as similar a format to a flat data frame as possible. This makes it easier to work with our data for two reasons. First, it lets us use {dplyr} data manipulations or join our spatial data with non-spatial data (some spatial data formats don’t let you do this). Second, it’s just simplier to wrap your head around. 12.3 Reading in Spatial Data Let’s have a look at some data. We can read in our data as a spatial object using the st_read funciton from the {sf} package (if it is saved in an .shp format and has necessary metadata). We’ll be working with some shapefiles which represent local authorities which are sourced from the ONS. Reading in Spatial Data LAs &lt;- sf::st_read(&quot;data/LAs&quot;) #&gt; Reading layer `LAs&#39; from data source #&gt; `/home/runner/work/R-cookbook/R-cookbook/data/LAs&#39; using driver `ESRI Shapefile&#39; #&gt; Simple feature collection with 382 features and 2 fields #&gt; Geometry type: MULTIPOLYGON #&gt; Dimension: XY #&gt; Bounding box: xmin: 364.2663 ymin: 9909.1 xmax: 655599.6 ymax: 1218502 #&gt; Projected CRS: Transverse_Mercator Looking at the message we get when we load in our data, we can find out some basic information about our sf dataframe: It tells us that it is a “Simple feature collection with 391 features and 10 fields”. In other words, this is telling us that it is an sf dataframe of 391 observations (features) and 10 variables (fields). The geometry type is “MULTIPOLYGON”. To break this down, the “POLYGON” is telling us we’re dealing with distinct shapes for each observation (as opposed to lines or dots), and the “MULTI” is telling us that one observation can be represented by multiple polygons. An example of this would be the Shetland Islands, a single local authority made up of many small polygons. The “bbox”” gives us the coordinates of a bounding box which will contain all the features of the sf data frame. The “epsg” and “proj4string” gives us the coordinate reference system. The most common code you will see is 4326 which refers to the “World Geodetic System 1984”. This is the coordinate reference system you are probably most familiar with (without realising it), as it is used in most GPS and mapping software. Points in the WGS system are referred to by their “latitude” and “longitude”. In this case, the epsg code is “NA”, in which case we will have to refer to the documentation to discern the correct epsg code to apply to the data. We can also ask R to give us all of this info with the st_geometry function. Inspect spatial data sf::st_geometry(LAs) #&gt; Geometry set for 382 features #&gt; Geometry type: MULTIPOLYGON #&gt; Dimension: XY #&gt; Bounding box: xmin: 364.2663 ymin: 9909.1 xmax: 655599.6 ymax: 1218502 #&gt; Projected CRS: Transverse_Mercator #&gt; First 5 geometries: #&gt; MULTIPOLYGON (((448874 536622.6, 453089.1 53411... #&gt; MULTIPOLYGON (((451744.3 520566.4, 451788.8 520... #&gt; MULTIPOLYGON (((478094.9 518835.6, 475360.6 516... #&gt; MULTIPOLYGON (((448472.8 525830.1, 448796 52591... #&gt; MULTIPOLYGON (((515719.3 428570.8, 515724.4 428... 12.4 CRS Projections There’s a lot that can be said about the utlity of different Coordinate Reference Systems, but long-story-short they all have unique advantages and disadvantages as they distort shape, distance, or size in different ways. In our day-to-day work, you’re most likely to encounter two coordinate reference systems: the World Geodatic System (WGS84 - EPSG code: 4326), or the Ordance Survey Great Britain 1936 system (OSGB36 - EPSG code: 27700). The WGS84 system is best suited to global data - for example plotting international ports or a world map of countries. It is a geodatic system, which means the points it defines refer to a point on a 3d elipsoid which represents the earth. The OSGB1936 system is best suited to mapping the UK or areas within it. It is a projected system, which means it projects the 3d curve of the earth into a 2d object. An advantage of this is that eastings (the x-axis of this projection) remains consistent at all northings (the y-axis). For example, a 1000 unit change in eastings is a 1km change anywhere, whereas a 1 unit change in longitude is a different distance depending on the latitude, as the earth gets narrower the further away you get from the equator. Complexities aside, practically what this means is that you will always want all of you’re spatial data to be in the same CRS, otherwise spatial joins and mapping functions will start throwing up errors. We can change the crs projection very easily with the {sf} function st_transform. Changing Coordinate Reference System LAs &lt;- sf::st_transform(LAs, crs = 27700) sf::st_crs(LAs) #&gt; Coordinate Reference System: #&gt; User input: EPSG:27700 #&gt; wkt: #&gt; PROJCRS[&quot;OSGB36 / British National Grid&quot;, #&gt; BASEGEOGCRS[&quot;OSGB36&quot;, #&gt; DATUM[&quot;Ordnance Survey of Great Britain 1936&quot;, #&gt; ELLIPSOID[&quot;Airy 1830&quot;,6377563.396,299.3249646, #&gt; LENGTHUNIT[&quot;metre&quot;,1]]], #&gt; PRIMEM[&quot;Greenwich&quot;,0, #&gt; ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], #&gt; ID[&quot;EPSG&quot;,4277]], #&gt; CONVERSION[&quot;British National Grid&quot;, #&gt; METHOD[&quot;Transverse Mercator&quot;, #&gt; ID[&quot;EPSG&quot;,9807]], #&gt; PARAMETER[&quot;Latitude of natural origin&quot;,49, #&gt; ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], #&gt; ID[&quot;EPSG&quot;,8801]], #&gt; PARAMETER[&quot;Longitude of natural origin&quot;,-2, #&gt; ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], #&gt; ID[&quot;EPSG&quot;,8802]], #&gt; PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996012717, #&gt; SCALEUNIT[&quot;unity&quot;,1], #&gt; ID[&quot;EPSG&quot;,8805]], #&gt; PARAMETER[&quot;False easting&quot;,400000, #&gt; LENGTHUNIT[&quot;metre&quot;,1], #&gt; ID[&quot;EPSG&quot;,8806]], #&gt; PARAMETER[&quot;False northing&quot;,-100000, #&gt; LENGTHUNIT[&quot;metre&quot;,1], #&gt; ID[&quot;EPSG&quot;,8807]]], #&gt; CS[Cartesian,2], #&gt; AXIS[&quot;(E)&quot;,east, #&gt; ORDER[1], #&gt; LENGTHUNIT[&quot;metre&quot;,1]], #&gt; AXIS[&quot;(N)&quot;,north, #&gt; ORDER[2], #&gt; LENGTHUNIT[&quot;metre&quot;,1]], #&gt; USAGE[ #&gt; SCOPE[&quot;Engineering survey, topographic mapping.&quot;], #&gt; AREA[&quot;United Kingdom (UK) - offshore to boundary of UKCS within 49°45&#39;N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.&quot;], #&gt; BBOX[49.75,-9,61.01,2.01]], #&gt; ID[&quot;EPSG&quot;,27700]] This changes the CRS projection from the WGS84 system to the OSGB1936 system, converting all of the coordinates to the new system. We can use the crs argument to change to any number of different projections. Note that this only works if the spatial object has data on the coordinate reference system already. If your data is missing all of this information, you will have to set the crs manually (if you know what the CRS projection should be): Setting Coordinate Reference System sf::st_crs(sf_object) &lt;- 27700 12.5 Manipulating Spatial Data As mentiond above, one of the key advantages of working with spatial data in {sf} format is that we can use tidyverse functions to manipulate our data. Let’s show this in action by creating a new variable. The variable “lad19cd” has a unique code for each local authority. All codes begin with a letter, followed by a numeric series. The letter refers to the country (“E” for England, “W” for Wales, “S” for Scotland, and “N” for Northern Ireland). We can use this to create a new variable called “country” for each feature. Manipulating {sf} data with dplyr LAs &lt;- LAs %&gt;% dplyr::mutate(country = dplyr::case_when(stringr::str_detect(lad19cd, &quot;W&quot;) ~ &quot;Wales&quot;, stringr::str_detect(lad19cd, &quot;S&quot;) ~ &quot;Scotland&quot;, stringr::str_detect(lad19cd, &quot;N&quot;) ~ &quot;Northern Ireland&quot;, stringr::str_detect(lad19cd, &quot;E&quot;) ~ &quot;England&quot;)) LAs %&gt;% as.data.frame() %&gt;% dplyr::group_by(country) %&gt;% dplyr::tally() country n England 317 Northern Ireland 11 Scotland 32 Wales 22 As we can see, we’ve manipulated our data in exactly the same way as we would have with a flat dataframe.We can also join spatial and non-spatial data together in the same way we normally would with dataframes. Let’s take some population data and join it to our Local Authorities. Joining Spatial and non-spatial data summary(LAs_pop) #&gt; lad19cd population #&gt; Length:437 Min. : 2242 #&gt; Class :character 1st Qu.: 104503 #&gt; Mode :character Median : 147997 #&gt; Mean : 960204 #&gt; 3rd Qu.: 276374 #&gt; Max. :66435550 #&gt; NA&#39;s :7 This dataframe of local authority populations has 2 variables, local authority code (lad19cd) and population. We can use a {dplyr} left_join to join this to our {sf} dataframe as we would with a flat dataframe. LAs &lt;- dplyr::left_join(LAs, LAs_pop, by = &quot;lad19cd&quot;) LAs$population &lt;- as.numeric(LAs$population) 12.6 Spatial Analysis - Joins A common task we might want to conduct is to manipulated a spatial object based on a relationship with another spatial object. To do this, we will use the st_join function from the {sf} package. The st_join allows us to join data on a number of different relationships, the simplest of which examines whether an object intersects another object - i.e. if there is any point where the geometries of both objects are identical. This could be where two lines cross, where two different spatial points objects have the same gemoetry, or it could be a point falling within the bounds of a polygon. Let’s look at some data representing the coordinates of road traffic accidents in Great Britain. Read in csv class(crash_data) #&gt; [1] &quot;data.frame&quot; names(crash_data) #&gt; [1] &quot;Location_Easting_OSGR&quot; &quot;Location_Northing_OSGR&quot; &quot;Date&quot; #&gt; [4] &quot;Number_of_Casualties&quot; Our crash data is in a flat csv format, BUT it has variables representing the easting and northing of where each acccident took place. We can use this to turn our crash data into an sf object with the function st_as_sf. We take our dataset, define which variables R should use to define coordinates in the “coords” argument, and define the “crs” that those coordinates are plotted in. Transforming flat data frames to spatial data crashes &lt;- crash_data %&gt;% dplyr::filter(!is.na(Location_Easting_OSGR) &amp; !is.na(Location_Northing_OSGR)) %&gt;% sf::st_as_sf(coords = c(&quot;Location_Easting_OSGR&quot;,&quot;Location_Northing_OSGR&quot;), crs = 27700) sf::st_geometry(crashes) #&gt; Geometry set for 122580 features #&gt; Geometry type: POINT #&gt; Dimension: XY #&gt; Bounding box: xmin: 84654 ymin: 10235 xmax: 655275 ymax: 1209512 #&gt; Projected CRS: OSGB36 / British National Grid #&gt; First 5 geometries: #&gt; POINT (529150 182270) #&gt; POINT (542020 184290) #&gt; POINT (531720 182910) #&gt; POINT (541450 183220) #&gt; POINT (543580 176500) The crashes sf dataframe holds information on all reported road traffic accidents in the UK since 2005. It has a number of variables for each accident, such as the accident severity, number of vehicles, number of casualties, time of day, and road and weather conditions. Let’s have a look at the most severe accidents (accidents with 5 or more casualties). Quick map of spatial data crashes %&gt;% dplyr::filter(Number_of_Casualties &gt;= 5) %&gt;% tmap::qtm() The qtm function is a useful function from {tmap} that let’s us quickly create maps with a minimum number of arguments. This is often useful for sense-checking data and can help you spot errors in data manipulation when working with spatial data. Here, for example, we can see that our data largely makes sense, so our conversion to {sf} was likely correct. We can (roughly) make out the shape of Great Britain, and we see some clusters around major cities (London, Birmingham, Manchester etc.). We can also see that we don’t appear to have data for Northern Ireland in this dataset (either that or Northern Irish drivers are much safer drivers). Let’s see can we work out which Local Authority has had the most road traffic casualties in this period. We can use a spatial join to assign a local authority to each road traffic accident. This works similarly to a {dplyr} left_join, with all of the variables from the y table being added to each observation of the x table which match eachother by a defined variable. The difference here is that the two tables are joined based on a shared geometry, rather than a shared value of a variable. The shared geometry here being whether a point intersects with a polygon (in other words, whether the point falls within a polygon). Joining spatial data crashes_join &lt;- sf::st_join(crashes, LAs, join = st_intersects) # essentially a left_join of LAs to crashes crashes_join$LA_name &lt;- as.character(crashes_join$LA_name) LAs_casualties &lt;- crashes_join %&gt;% dplyr::group_by(LA_name) %&gt;% dplyr::summarise(casualties = sum(Number_of_Casualties)) # making an sf data frame called LAs_casualties which is the result of a simple group_by and summarise. LAs_casualties %&gt;% as.data.frame() %&gt;% dplyr::arrange(desc(casualties)) %&gt;% select(LA_name, casualties) %&gt;% head() LA_name casualties Birmingham 3592 Leeds 1980 Westminster 1883 Cornwall 1663 Lambeth 1402 NA 1340 We can see that Birmingham has had more casualties in this period than any other local authority. The LAs_casualties dataset that we created above has the same local authorities names as our original LAs sf dataframe, meaning we can easily left_join this table to our original LAs_shapefile. But first, we will turn it into a flat dataframe (ie, with no geographic information) with the st_set_geometry function. If we did not get rid of the geometry before left_joining, we would create a “geometry_collection” object for each local authority, which would be a mess of polygons and points within those polygons for each feature. This is because our group_by function above also grouped together the geometries of the observations it was grouping together. If we look at the outout above, we can see that the geometry type of LAs_casualties is “MULTIPOINT”, so the “Birmingham” observation has 3,532 points representing where each road accident happened, rather than outlining the polygon of Birmingham in any meaningful way. LAs_casualties &lt;- LAs_casualties %&gt;% sf::st_set_geometry(NULL) LAs &lt;- dplyr::left_join(LAs, LAs_casualties, by = &quot;LA_name&quot;) We now have our casualties data joined to our original local authorities shapefile. We can now use this to make graphical representations of our data. 12.7 Plotting and simplifying Let’s have a look at one of our Local Authorities in isolation, Cornwall. We can see that our polygon is quite detailed. While this detail is desirable for when we conduct our spatial analysis, if we want to plot all of our local authorities at the same time, it can be better to plot simplified polygons. This is for 3 reasons. First, simplification means that R can create and output plots quicker; Second, outputs will have a smaller file size (particularly useful for interactive plots or when we share our outputs); and third, sometimes simplified polygons actually look better in plots as too-detailed borders can come out looking murky due to over-plotting, particularly in static plots. To alleviate this, we can simplify our geometry features before plotting. For this, we will use the function ms_simplify from the package {rmapshaper}. This function simplifies spatial data while maintaining key topology. Simplifying functions in other packages can do funny things such leave gaps between polygons with internal borders when simplifying. ms_simplify doesn’t do this. Simplifying Spatial Data LAs_simp &lt;- rmapshaper::ms_simplify(LAs, keep = 0.05, keep_shapes = TRUE) This command creates a new sf data frame called “LAs_simp”, which has all the same information as the original LAs sf dataframe, but has a simplified geometry with fewer points. The “keep” argument specifies how much geographic information we want to hold onto. Here we only keep 5% of the original points. The “keep_shapes” argument specifies whether or not we want to allow the function to delete some features entirely for simplification purposes. This can be useful for polygons with lots of small islands as individual entries (where you are indifferent to whether or not you keep all islands in your dataset), BUT use with caution, as if you simplify too much R might delete entire features (observations) to fulfill the simplification. Here we set “keep_shapes = TRUE”” so we don’t lose any features. Let’s have a look at the difference between our original and our simplified polygon: Even with only 5% of the original points, we can see that we’ve held onto a lot of the information that we’d want if we were plotting these polygons. However, we should always use simplification with caution, and only simplify our spatial objects after we’ve carried out spatial analysis. Note that we can also simplify spatial data which are lines, reducing the number of vertices in lines, but we cannot simplify spatial points data, as their geometries (a single point for each entry) is already as small as it can be. 12.8 Mapping Spatial Data There are a number of packages that you can use to make maps with spatial data in R, including {leaflet}, {ggmap}, {mapdeck}, and {ggplot2}. Here, we will be using the package {tmap}. {tmap} is a highly versatile package for mapping spatial data, with a number of advantages over other packages: {tmap} uses a similar grammar to {ggplot2}, where aesthetic layers are layered on top of each other; the same piece of {tmap} code can be used to make either static or interactive maps, and; {tmap} can work with spatial data in a number of different formats, including {sp}, {raster}, {geojson} and {sf}. Similar to using {ggplot2}, the first layer of a tmap defines the spatial object that we wish to plot in the function tm_shape. We then add (+) an aesthetic element based on this spatial object Most often this aesthetic element will be one of type tm_polygons, tm_line, or tm_dots, depending on the spatial data type. Let’s see this in action, and map our local authorities based on how many road traffic casualties there were in each. Create static map in tmap LAs &lt;- rmapshaper::ms_simplify(LAs, keep = 0.05, keep_shapes = TRUE) # Simplifying polygons. Remember to only do this after you&#39;ve finished your spatial analysis, or to save the simplified version as a different name. tmap::tm_shape(LAs) + # defining what shapefile to use tmap::tm_polygons(col = &quot;casualties&quot;, # setting the variable to map the colour aesthetic to. tmap takes variable names in quotation marks style = &quot;quantile&quot;, # the style option lets us define how we want the variable split n = 5, # number of quantiles pal = &quot;YlGnBu&quot; # choose from a number of palettes available from rcolorbrewer, or define your own ) + tmap::tm_borders(col = &quot;#000000&quot;, # defining polygon border as black lwd = 0.3 # setting border width ) + tmap::tm_layout(legend.outside = TRUE # putting the legend outside the plot rather than inside ) One problem with this plot is that we still have Northern Ireland in the plot, and we only have NAs for northern Ireland (as our casualties dataset didn’t have accidents which happened there). To fix this, we can utilise a simple {dplyr} filter to remove features which have an NA value for casualties. We can then pipe a filtered sf dataframe into the tm_shape function. LAs %&gt;% dplyr::filter(!is.na(casualties)) %&gt;% tm_shape() + # defining what shapefile to use tm_polygons(col = &quot;casualties&quot;, # setting the variable to map the colour aesthetic to. tmap takes variable names in quotation marks style = &quot;quantile&quot;, # the style option lets us define how we want the variable split n = 5, # number of quantiles pal = &quot;YlGnBu&quot; # choose from a number of palettes available from rcolorbrewer, or define your own ) + tm_borders(col = &quot;#000000&quot;, # defining polygon border as black lwd = 0.3 # setting border width ) + tm_layout(legend.outside = TRUE # putting the legend outside the plot rather than inside ) Though this plot is useful, and gives us a sense of the distribution of accidents, it’s difficult to make out values of smaller local authorities. What would make this easier is if we changed our map to an interactive map. We can do this very easily, using the same chunk of code, by simply setting the tmap_mode to “view”. Create interactive map in tmap tmap_mode(&quot;view&quot;) LAs %&gt;% filter(!is.na(casualties)) %&gt;% tm_shape() + # defining what shapefile to use tm_polygons(col = &quot;casualties&quot;, # setting the variable to map the colour aesthetic to. tmap takes variable names in quotation marks. style = &quot;quantile&quot;, # the style option lets us define how we want the variable split n = 5, # number of quantiles pal = &quot;YlGnBu&quot; # choose from a number of palettes available from rcolorbrewer, or define your own ) + LAs %&gt;% filter(!is.na(casualties)) %&gt;% tm_shape() + tm_borders(col = &quot;#000000&quot;, # defining polygon border as black lwd = 0.3 # setting border width ) + tm_layout(legend.outside = TRUE # putting the legend outside the plot rather than inside ) It’s worth noting that some arguments in {tmap} are only availble in static plots, and some only available in interactive maps. However, R will just skip the arguments that don’t apply, so it wont break your code to leave them in. Here, the “legend.outside” argument is meaningless in an interactive plot, so is skipped by R when creating this interactive plot. A problem with this interact plot is that when you click on a local authority, you can’t find out it’s name. The default popup variable is the object id, because it’s the first variable in the sf dataframe, which isn’t very useful. We can manually set the popup variables instead. We can also make the map look a bit nicer by increasing the transparency of the polygons with the “alpha” argument. Finally we can pick from any number of basemaps. {tmap} leverages the {leaflet} package for basemaps, and you can find a list of available basemaps here. Improving Interactive Map tm_basemap(server = &quot;CartoDB.Positron&quot;) + # defining basemap LAs %&gt;% filter(!is.na(casualties)) %&gt;% tm_shape() + # defining what shapefile to use tm_polygons(col = &quot;casualties&quot;, # setting the variable to map the colour aesthetic to. tmap takes variable names in quotation marks. style = &quot;quantile&quot;, # the style option lets us define how we want the variable split n = 5, # number of quantiles pal = &quot;YlGnBu&quot;, # choose from a number of palettes available from rcolorbrewer, or define your own alpha = 0.4, # setting transparency popup.vars = c(&quot;Local Authority&quot; = &quot;lad19nm&quot;, &quot;casualties&quot;), # setting variables to be seen on click id = &quot;lad19nm&quot; # Setting a variable to display when hovered over ) There are myriad aesthetic possibilities in {tmap} and related packages. It’s not uncommon for a map to be built first in {tmap}, before exported as {leaflet} map and further edited in the {leaflet} package, though we wont cover that here. It’s also possible to plot more than one spatial object at the same time. For example, layering polygons, points and lines from various shapefiles. Simply define the new spatial object with tm_shape and add aesthetics as before. Let’s see this in action, adding a layer to our map with the worst car accidents in our dataset (accidents with over 10 casualties). Mapping multiple spatial objects tm_basemap(server = &quot;CartoDB.Positron&quot;) + # defining basemap LAs %&gt;% filter(!is.na(casualties)) %&gt;% tm_shape() + # defining what shapefile to use tm_polygons(col = &quot;casualties&quot;, # setting the variable to map the colour aesthetic to. tmap takes variable names in quotation marks. style = &quot;quantile&quot;, # the style option lets us define how we want the variable split n = 5, # number of quantiles pal = &quot;YlGnBu&quot;, # choose from a number of palettes available from rcolorbrewer, or define your own alpha = 0.4, # setting transparency popup.vars = c(&quot;Local Authority&quot; = &quot;lad19nm&quot;, &quot;casualties&quot;), id = &quot;lad19nm&quot;) + crashes %&gt;% filter(Number_of_Casualties == 10) %&gt;% tm_shape() + # defining our second shapefile to use and plot in the same output as above tm_dots(col = &quot;red&quot;, # calling tm_dots on point objects alpha = 0.7, # making the dots partially transparent jitter = 0.05, # adding a small amount of random jittering to points so overlayed points are visible id = &quot;Number_of_Casualties&quot; ) 12.9 Mapping with {leaflet} The {leaflet} package can be used to create interactive maps in R. Similar to {ggplot2}, you start with a base map and then add layers (i.e. features). We’re going to map some search and rescue helicopter data, using the longitude and latitude. #&gt; Rows: 209 Columns: 7 #&gt; ── Column specification ──────────────────────────────────────────────────────── #&gt; Delimiter: &quot;,&quot; #&gt; chr (5): Base, Unique_ID, Cat, Place, Domain #&gt; dbl (2): latitude_new, longitude_new #&gt; #&gt; ℹ Use `spec()` to retrieve the full column specification for this data. #&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Base Unique_ID Cat latitude_new longitude_new Place Domain Sumburgh Sumb6559171 Rescue/Recovery 61.10333 1.073333 Cormorant Alpha Maritime Caernarfon Caer6580171 Support 51.88602 -5.307241 Whitesands Bay Coastal Humberside Humb6587171 Rescue/Recovery 53.39779 -1.903712 Kinder Scout Land Prestwick Pres6613171 Rescue/Recovery 56.33115 -4.618574 Beinn A Chroin Land Inverness Inve6614171 Rescue/Recovery 56.80076 -5.034575 Ben Nevis Land Newquay Newq6617171 Search only 50.04000 -5.636667 Porthcurno Maritime More information on using {leaflet} in R can be found here: https://rstudio.github.io/leaflet/ 12.10 Other Spatial Types We have worked only with the {sf} package here, but it’s important to be aware of different types of spatial data formats that exist in R. Most similar to the {sf} package is {sp}. sp objects hold the different spatial and non-spatial components of a dataset in different “slots”, creating a hierarchical data structure, where an observation in one slot is linked to an observation in another slot. This works similarly to a relational database in SQL. For example, if our local authorities were in sp format, we would access names of local authorities by calling something like “LAs_sp@data\\(lad19nm&quot;. The &quot;@&quot; symbol specifies we want the dataframe &quot;slot&quot;, and then we call the variable with the &quot;\\)” as usual. Other slots in an sp object are the bounding box, the proj4string (a way of defining the projection, essentially a long-form crs), and the spatial data (points, polygons etc.). A drawback of holding data like this is that you have to use base R to manipulate data. {dplyr} will not work with sp objects, making sp objects more cumbersome to work with. {sf} was developed to replace {sp}, but as {sf} is a relatively new package, there may be times when you encounter a function you need that only works on an {sp} object. When this happens, we can quickly convert objects back and forth between sf and sp with the following commands: Converting data from sf to sp objects object_sp &lt;- as(object_sf, class = &quot;SPATIAL&quot;) # sf to sp object object_sf &lt;- st_as_sf(object_sp) # sp to sf object Another type of spatial data package to be aware of is the {raster} package. Raster objects are in a gridded data format: they define a rectangle, with a point of origin and an end point, and then define the number of rows and columns to split that rectangle into, making a grid of equal cell sizes. This format spares R from having to plot a polygon for every single grid square. It’s often used for altitude or temperature data, but can be used for a wide range of data (eg emissions, or calculating distance from features of interest across a defined spatial area). Raster objects are held in the {raster} package, though plotting can be done with {tmap}, {leaflet}, and a range of mapping packages. 12.11 Further Resources For a range of shapefiles, the ONS data portal is a good place to look. It has local authorities with joinable data on average wages, populations, etc. as well as a number of other shapefiles for points/areas of interest such as national parks, hospitals, output areas of different sizes, and rural/urban areas. For resources on using tmap and other mapping functions, see this online workshop which covers a range of topics. It should take about 4 hours to do everything in the workshop (but also good to just find individual bits of code). It mostly uses {ggplot2} but it finishes with some work with {raster} and {tmap}. There’s also this very good blog post, which does a good job of peaking under the hood of how sf and sp objects work without getting too technical. For a more detailed overview of mapping in R, see this amazing online book from Robin Lovelace. Of particular interest are Chapters 5 and 8, but the whole book is useful. "],["functions.html", "Chapter 13 Functions 13.1 User defined functions (UDFs) 13.2 Top tips for writing functions 13.3 Function structure 13.4 Function with purrr structure 13.5 Packages within DfT 13.6 Further reading", " Chapter 13 Functions This chapter contains the basics of how to write a function, links to more complete instructions, and examples of functions. 13.1 User defined functions (UDFs) A non-exhaustive list of possible reasons for writing your own functions; you are repeating some code chunk changing only a few variables each time. you want to apply the same set of commands to multiple instances of some object: it might be a dataframe, a text string, a document, an image etc. tidier code, you can write the functions in a separate R file and load these functions by running that file when you load other packages. easier to update, just update the function rather than edit potentially multiple code chunks. prevents copy/paste errors you are writing a package/library 13.2 Top tips for writing functions name the function something sensible that gives you a clue what it does, usually containing a verb, and making sure that it doesn’t conflict with names of other functions in use. write some description of what the function is intended for and the expected outcome, including what it needs to work properly, eg it might take an integer value argument only, and therefore fail if given a double. make your function as independent as possible, use name_space::function syntax if it uses functions from other packages so they don’t have to be loaded (they will have to be installed though). recommend passing named arguments rather than relying on global variables (again name your arguments clearly). recommend giving arguments default values, this shows an example of the type of variable that is expected here. note that variables assigned inside a function will not appear in the global environment unless explicitly returned. using an explicit return statement is useful if the function is complex, otherwise the output will be the final evaluated expression. 13.3 Function structure Below is an example of the syntax for defining a function named state_animal_age. The list of named arguments are included in brackets () with defaults. The function code sits inside braces {}, and returns a single string of the concatenated inputs. state_animal_age &lt;- function(animal = &quot;animal type&quot;, age = 0) { #function takes animal name as a string and current age and pastes them together #output is single string return( paste(animal, age, sep = &#39;: &#39;) ) } #run function with defaults state_animal_age() #&gt; [1] &quot;animal type: 0&quot; #run function passing inputs to argument names, order doesn&#39;t matter state_animal_age(age = 2, animal = &quot;potoo&quot;) #&gt; [1] &quot;potoo: 2&quot; #run function passing inputs by position, order matters state_animal_age(&quot;tortoise&quot;, 169) #&gt; [1] &quot;tortoise: 169&quot; 13.4 Function with purrr structure The scenario in which I most find myself writing functions is when I want to do the same thing to a set of similar objects. I write a function that does what I want to one object and then use purrr::map (or one of its siblings) to iterate through the complete list of objects. This greatly simplifies the entire process. For examples see Chapter 3 .xlsx and .xls section and below. Using the function above that takes two inputs I can use purrr::map2 to iterate over two lists of these input values. Note that the corresponding values in each list form a pair of inputs for the function, so we get three outputs. Consider how this is different to iterating over nested lists, where, in this case, we would get nine outputs. animal_list = c(&#39;cat&#39;, &#39;dog&#39;, &#39;elephant&#39;) age_list = c(12, 7, 42) purrr::map2(.x = animal_list, .y = age_list, .f = state_animal_age) #&gt; [[1]] #&gt; [1] &quot;cat: 12&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;dog: 7&quot; #&gt; #&gt; [[3]] #&gt; [1] &quot;elephant: 42&quot; 13.5 Packages within DfT In addition to writing your own functions, there are a number of packages available for you to use in DfT. As good practice, you should always aim to use an existing package when one is available rather than writing your own functions, as this ensures your code remains maintained and consistent. DfT packages currently available include: dftplotR Create charts with accessible DfT-themed formatting and colour palettes slidepackR Create DfT-themed HTML slidepacks in R tableformatR Format openxlsx tables to make them accessible for publication mojspeakr Technically not a DfT package, but widely used here! Helps to convert Rmarkdown to a format suitable for publication on gov.uk 13.6 Further reading Functions chapter of R for Data Science by Hadley Wickham "],["qa.html", "Chapter 14 Quality assurance in code 14.1 QA of data 14.2 QA of code", " Chapter 14 Quality assurance in code This chapter includes best practice advice for quality assurance and code, including both using R to carry out QA checks, and quality assurance of code itself. 14.1 QA of data Quality assurance of data is an obvious choice for something that can be partially automated as part of your coding. Basic checks of aspects such as data size and structure, totals of data matching and consistency of figures across multiple outputs are time consuming to check manually but require little analytical insight, so can generally be fully automated. In contrast, checks requiring a more in-depth knowledge such as ensuring changing figures and trends are feasible are often better partially automated; producing charts or other visualisations to allow an analyst to assure the data using their own knowledge. 14.1.1 Adding clear messages to functions While good code makes use of functions, when complex data manipulation is performed inside a function it can often be difficult to see when or how errors arise. To make this easier, you should make use of errors, warnings and messages inside of functions, which provide the user information when something goes wrong. Error messages will stop the function from running and produce an error as its only output. This is particularly useful when you are checking something that you know will completely break the functionality of the function; e.g. a file is not found, and you want it to stop running completely. In contrast, warning messages do not stop the function from running, but will produce a warning alongside the result, indicating to the user that there may be a problem. This is particularly useful when you know that failing a check is likely to result in data you can’t use, but you still want the option to run the code inside the function. Finally, messages can be used to provide the user information only. This is useful when you want to provide information about what is going on in the function, even when this is not associated with a negative check. These should not be the only checks you rely on for quality assurance however. As they only produce a message in the console, it is possible to miss these messages, and there is no long term storage of any error messages which arise. This makes it difficult to audit errors or discover when they first started as there is no record of quality checks performed. 14.1.2 Generating QA reports A clear and auditable way to carry out QA checks on data is to produce simple QA reports in RMarkdown. These reports can include: Simple yes/no checks with verbal responses: #&gt; [1] &quot;Unexpected number of columns in mtcars: 13&quot; Returning only specific (e.g. unexpected or unusual) values in a dataset for further checking Returning charts of data to visually inspect for anomalies Rendering in HTML format also produces a report which is easy to read in multiple formats and cannot be modified. Reports can be rendered as part of a larger automated project by using the rmarkdown::render() function. This function offers two advantages over using the knit button: Doesn’t require manual intervention to produce the report Using the output_file argument in the render call allows you to specify a dynamic file name for the output report. This allows you to produce a unique report named for the date or time it was run, producing an auditable trail of QA reporting. Additional arguments within the render call also allow you to specify: The output directory, allowing you to output to a different folder Output format; can specify different and multiple output types Pass parameters to the knitted rmarkdown; this is ideal to run the same report multiple times with only small changes (e.g. filename of data to check, quarter of data to check) 14.2 QA of code In addition to using code to perform quality checks on data sources, it’s important that code itself is quality assured. As automated steps will be repeated frequently, they can be a source of significant errors if it hasn’t been checked that the code is fit for purpose and robust. 14.2.1 Code commenting Adding clear code comments is one of the easiest ways to ensure that your code remains fit for purpose. Comments should explain the why of your code; i.e. what it is doing in plain language and why you are doing this. Clear code comments allow reviewers to check that the code is doing what it is supposed to do, and to suggest better options to achieve that aim, even if they are not familiar with the project itself. 14.2.2 Code reviews You should get into the habit of your code being regularly reviewed by your peers, to ensure that it is good quality, efficient and robust. The easiest way to build this into your workflow is to make use of the code reviewing tools in Github. As well as allowing you to leave line-by-line comments on code, Github also makes it easy to request review on every pull request by default, and keeps an auditable record of all your code reviews alongside your code. Further information about how code reviews on Github work and how to perform a good code review. 14.2.3 Testing code Unit testing ensures that functions in your code still work as expected every time you run them. This is ideal for code which you know may be updated in future, but also any code which is essential for the running of your project. You probably run unofficial unit tests every time you write new code; steps such as running code and then examining the output to check the data type, number of columns or data structure are examples of unit tests. Writing tests using a package such as testthat formalises that process and allows those tests to be run repeatedly to spot problems before they occur. Read more about unit testing in R and the testthat package "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
